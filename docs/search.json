[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Forzán",
    "section": "",
    "text": "This webpage is done with the objective of advertise myself for companies and individuals to make me more hirable — or at least appealing. Think of it as a second résumé; however I would add the prefix anti-résumé because I also seek feedback, discussion, community to get better in my professional career.\nI am really interested in the world of modelling, biotech, engineering, technology, industry, and in a way philosophy (the fact of doing a mathematical model has many implications about reality itself). I’ll be posting about these topics.\nEmail: isaac_forzan@outlook.com"
  },
  {
    "objectID": "index.html#hola",
    "href": "index.html#hola",
    "title": "Isaac Forzán",
    "section": "",
    "text": "This webpage is done with the objective of advertise myself for companies and individuals to make me more hirable — or at least appealing. Think of it as a second résumé; however I would add the prefix anti-résumé because I also seek feedback, discussion, community to get better in my professional career.\nI am really interested in the world of modelling, biotech, engineering, technology, industry, and in a way philosophy (the fact of doing a mathematical model has many implications about reality itself). I’ll be posting about these topics.\nEmail: isaac_forzan@outlook.com"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "",
    "text": "Strangely enough, two years ago I was wandering in the college library thinking “what do quality engineers do?”; in my mind it’s common for me to be wondering that given the nature of my undergraduate degree (biotech engineer). Reading about it I stumbled with terms like Lean Six Sigma, DMAIC, SKUs, KPIs, etc…, but I wasn’t compelled because I read a word that made me turn my stomach. Gurus. I couldn’t handle that word at the time. “How can a serious book for engineers follow someone known as a guru” I thought. I wouldn’t have imagined that 2 years from that moment I would be digging into philosophies from these gurus — being Juran’s my favorite.\nA big part of quality is Inventory management, and that’s the topic for my second blog. This is the objective: teach you how to make a simulation of a beer business model for its inventory management using R + tidyverse. You cannot be vague nor lie to a computer; so if you program it, you know it.\nThis blog is heavily influenced by Vandeput (2020) fantastic book about inventory optimization — I am half through it. If you want get into inventory theory I fully recommend this book, he really explains it phenomenally.\nWe will be using the next packages:\n\n\nShow code\nlibrary(tidyverse) # for data manipulation, plotting, etc...\nlibrary(gt)        # for better tables\nlibrary(igraph)    # for graphs"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#inventory-policy-periodic-review-order-up-to-level-r-s",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#inventory-policy-periodic-review-order-up-to-level-r-s",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "Inventory policy: Periodic Review & Order Up-to Level (R, S)",
    "text": "Inventory policy: Periodic Review & Order Up-to Level (R, S)\nWhile there are many policies, Dr.House Brewer has a very popperian philosophy and does not trust new innovative policies that promise to increase its ROI by 100%, NO, the company adopts the common Periodic Review & Order Up-to Level policy. If you go to Walmart on a specific day of the week to stock up on groceries, you are following Periodic Review & Order Up-to Level policy. It consists of a periodic time interval (say every 5 workdays), \\(R\\), when you reorder all your inventory with your suppliers (fixed schedule) and an up-to level reorder point, \\(S\\) (you always want to have 10 eggs, you have right now 2, you wait until it is Monday (your reorder time, \\(R\\)) to buy your 8 eggs; 10 eggs is your up-to level reorder point, \\(S\\)). This policy is usually written as \\((R,S)\\), \\(R\\) for review period and \\(S\\) for up-to level reorder point.\n\nSimulation time: (R, S) policy in R\nWe will assume two things:\n\nThe demand of the customers, \\(d\\), is constant/fixed\nThe lead time for the materials to arrive at the warehouse, \\(l\\), does NOT exist\n\nThese are strong assumptions that are going to be a key point latter in this blog. Let’s think of \\(l\\); if there is no lead time, \\(l = 0\\), the moment it is your reorder time, \\(R\\), is the moment you get it. This is never the case, but we are going to assume it for the moment.\nWe need to define the demand, \\(d\\), the time period (in this daily case), \\(R\\) and \\(S\\), so…,\n\n\nShow code\n# Parameters:\ns        &lt;- 14  # order-up-to level (14 units)\nr_period &lt;- 5   # review period (every 5 days)\ndemand   &lt;- 2   # constant daily demand\nn_days   &lt;- 24  # number of days to simulate\n \n# Initialize vectors for the simulation\ndays         &lt;- 1:n_days\ninventory    &lt;- numeric(n_days)  # the storage of the data\ninventory[1] &lt;- s                # Start with order-up-to level\n\n\nFantastic, we want a plot of how inventory changes over time using the (R, S) policy, so it will be our dependent variable. I’ll show you the code, try to understand it, then I’ll explain the logic behind it:\n\n\nShow code\nfor (i in 1:n_days) {\n  if ((i - 1) %% r_period == 0 && i &gt; 1) { # \"%%\"-&gt; modulus operation so if 7 %% 3 = 1\n    # Replenish to order-up-to level, s:\n    inventory[i] &lt;- s                 \n  } else if (i &gt; 1){\n    # Carry forward previous day's ending inventory:\n    inventory[i] &lt;- inventory[i - 1]  \n  }\n  \n  # Consume demand at end of the day:\n  inventory[i] &lt;- inventory[i] - demand\n  # print it\n  cat(\"Day\", i, \": Inventory after demand =\", inventory[i], \"\\n\")\n}\n\n\nDay 1 : Inventory after demand = 12 \nDay 2 : Inventory after demand = 10 \nDay 3 : Inventory after demand = 8 \nDay 4 : Inventory after demand = 6 \nDay 5 : Inventory after demand = 4 \nDay 6 : Inventory after demand = 12 \nDay 7 : Inventory after demand = 10 \nDay 8 : Inventory after demand = 8 \nDay 9 : Inventory after demand = 6 \nDay 10 : Inventory after demand = 4 \nDay 11 : Inventory after demand = 12 \nDay 12 : Inventory after demand = 10 \nDay 13 : Inventory after demand = 8 \nDay 14 : Inventory after demand = 6 \nDay 15 : Inventory after demand = 4 \nDay 16 : Inventory after demand = 12 \nDay 17 : Inventory after demand = 10 \nDay 18 : Inventory after demand = 8 \nDay 19 : Inventory after demand = 6 \nDay 20 : Inventory after demand = 4 \nDay 21 : Inventory after demand = 12 \nDay 22 : Inventory after demand = 10 \nDay 23 : Inventory after demand = 8 \nDay 24 : Inventory after demand = 6 \n\n\nLogic: our inventory array will be decreasing per day given the constant demand, but given that we have a periodic review, \\(R\\), the moment it is \\(R\\)-day the moment it replenishes your inventory (remember we won’t be waiting any time between order and the supplies arriving at the warehouse, there is no lead time) to an up-to level reorder point, \\(S\\). We need a for-loop that will help us evaluate whether the inventory per i day. A day is composed of mornings and evenings; in the morning we evaluate whether it is \\(R\\)-day or not, this is what the first condition does (i - 1) %% r_period; inside the loop:\n\n\n\n\n\n\n\n\n\nDay\nEvaluation\nResult\nNote\n\n\n\n\ni = 1\n(1-1) %% 5 = 0\nFalse\nbut we exclude day 1 with && i &gt; 1\n\n\ni = 2\n(2-1) %% 5 = 1\nFalse\n-\n\n\ni = 3\n(3-1) %% 5 = 2\nFalse\n-\n\n\n.....\n.....\n.....\n.....\n\n\ni = 6\n(6-1) %% 5 = 0\nTrue\nequal to 0\n\n\n\nThe next conditional, else if (i &gt; 1) , is just saying “Is this NOT the first day AND NOT a review day? If YES, start with yesterday’s leftover inventory”. Then by the end of the day we have inventory[i] - demand inventory.\n\n\n\n\n\n\nNote\n\n\n\nYou can make this for-loop a function with simulate_inventory &lt;- function(s, r_period, demand, n_days) as parameters, if you want.\n\n\n\n\nShow code\n# Create a tidy data fram, called tibble\ninventory_data &lt;- tibble(\n  days = days,\n  inventory = inventory\n)\n# Plot the inventory over time\nplot_1 &lt;- inventory_data |&gt; \n  ggplot(aes(days, inventory)) +\n  geom_line(size = 1, color = \"darkblue\", linetype = \"dashed\") +\n  geom_point(size = 2, color = \"red\") +\n  labs(\n    title = \"Inventory Levels Over Time\",\n    subtitle = paste(\"(R,S) Policy: Review every\", r_period, \"days, Order up to\", s, \"units\"),\n    x = \"Days\",\n    y = \"Inventory Level\"\n  ) +\n  scale_x_continuous(breaks = seq(0, n_days, 5)) +\n  theme_bw() +\n  geom_vline(xintercept = seq(5, n_days, r_period), \n             linetype = \"dashed\", alpha = 0.5, color = \"black\") +\n  annotate(\"text\", x = seq(5, n_days, r_period), y = s + 1, \n           label = \"Reorder\", size = 3, angle = 90, vjust = -0.5)\nplot_1 \n\n\n\n\n\n\n\n\nFigure 1: (R,S) simulation. Note how it never gets S = 14 [units]; because the simulation gives the inventory AFTER the review, or at the end of the day. So after R-day next day in the morning you have 14 but you will be consuming 2, given the daily demand, which makes it 12 [units] in the inventory\n\n\n\n\n\nFigure 1 shows the plot of inventory vs time. Note how the assumptions affect the simulation: there is an instantaneous replenishment of the inventory level and the inventory level is decreasing uniformly (just a straight line) given constant daily demand. This policy does not show fluctuations in time, but it can be risky because you can get into red levels of inventory level given an unexpected increase in demand and being far from \\(R\\)-day (or any given time-period).\nDr.House Brewer has chosen its policy, how can it be optimized?\n\n\n\n\n\n\nNote\n\n\n\nInventory Level is the net inventory: available on-hand inventory and in-transit inventory, minus back-orders, orders not yet shipped, etc."
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#economic-order-quantity-eoq-model",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#economic-order-quantity-eoq-model",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "Economic Order Quantity (EOQ) model",
    "text": "Economic Order Quantity (EOQ) model\nHow much should I order? turned out to be depending on the costs!!! — it is always about money.\n\\[\nC(Q)\n\\]\nWhere \\(C\\) is cost, and \\(Q\\) is order quantity. There is two types of costs:\n\nHolding costs. Costs associated with storing products. Two types:\n\nVariable holding costs (e.g. losses)\nFixed holding costs (e.g. employees)\n\nTransaction Costs. All costs associated with ordering the product from the suppliers. Not only transportation. Two types:\n\nVariable transaction costs (e.g. packaging)\nFixed transaction costs (e.g. fixed fees)\n\n\nWe won’t be caring for Variable transaction costs because they do not depend on \\(Q\\) (they depend on the demand, \\(d\\)). This is the equation:\n\\[\nTotal Costs = Holding Costs + Transaction Costs\n\\]\n\\[\n\\longrightarrow C(Q) = h \\frac {Q}{2} + k \\frac{D}{Q}\n\\]\n\n\n\n\n\n\nDimensional Analysis for the engineers out there:\n\n\n\n\\[\nC(Q) = \\left( \\frac {[currency]} {[unit]  [time]} * \\frac {[item]}{1} \\right) +  \\left( \\frac{[currency]}{[order]} * \\frac {[unit]}{[time]} * \\frac{1}{[unit]}      \\right)\n\\]\nSince \\([order]\\) is a count, it is often considered dimensionally defined as equivalent to a pure number, dimensionless.\n\\[\nC(Q) = \\left( \\frac {[currency]} {[unit]  [time]} * \\frac {[item]}{1} \\right) +  \\left( \\frac{[currency]}{1} * \\frac {[unit]}{[time]} * \\frac{1}{[unit]}      \\right)\n\\]\n\\[\n= \\frac{[currency] }{[time]}  +  \\frac{[currency] }{[time]}  = \\frac{[currency] }{[time]}\n\\]\n\n\n\nWhat does it all mean?\nI won’t be focusing on the code, for the moment (it is relatively easy this chunk of code), because it isn’t really important. What is important is understanding the EOQ equation (\\(C(Q)\\)). So let’s focus on the plots:\n\n\nShow code\n# EOQ: total cost = holding costs + transactions costs\nh &lt;- 0.2\nk &lt;- 0.5 \n\neoc_m &lt;- tibble(\n    q = seq(1, 10, 0.1),         # Order quantity\n    h_cost = (h) * (q/2),        # Holding cost\n    t_cost = k * (demand/q),     # Transaction cost\n    total_cost = h_cost + t_cost # Total costs\n  ) \n\nplot_2 &lt;- eoc_m |&gt; \n  ggplot(aes(q, h_cost)) +\n  geom_line(color = \"darkorange\", size = 2) +\n  theme_bw() +\n  labs(\n    title = \"Holding cost: (h) * (q/2)\",\n    x = \"Cost\",\n    y = \"Order quantity\"\n  )\n\nplot_3 &lt;-  eoc_m |&gt; \n  ggplot(aes(q, t_cost))+\n  geom_line(color = \"cyan3\", size = 2) +\n  theme_bw() +\n  labs(\n    title = \"Transaction costs:  k * (demand/q)\",\n    x = \"Order quantity\",\n    y = \"Cost\"\n  )\n\n# Make it in long format for the labels in the ggplot\nplot_4 &lt;- eoc_m |&gt; pivot_longer(\n  cols = c(h_cost, t_cost, total_cost), \n  names_to = \"cost_type\", \n  values_to = \"cost_value\") |&gt; \n    mutate(cost_type = factor(cost_type, \n                              levels = c(\"h_cost\", \"t_cost\", \"total_cost\"),\n                              labels = c(\"Holding Cost\", \"Transaction Cost\", \"Total Cost\"))) |&gt; \n    ggplot(aes(x = q, y = cost_value, color = cost_type)) +\n    geom_line(size = 1) +\n    scale_color_manual(values = c(\"Holding Cost\" = \"darkorange\",\n                                  \"Transaction Cost\" = \"cyan3\",\n                                  \"Total Cost\" = \"dodgerblue4\")) +\n    theme_bw() +\n    labs(\n      x = \"Order quantity\",\n      y = \"Costs\",\n      color = \"Cost Type:\",\n      title = \"Economic Order Quantity (EOQ) Cost Analysis\"\n    ) +\n    geom_vline(xintercept = c(3, 3.5), \n               linetype = \"dashed\", alpha = 0.5, color = \"red\") +\n    annotate(\"text\", x = 3.3, y = 0.68, color = \"red\",\n             label = \"Optim. Area\", size = 3, vjust = -0.5) +\n    theme(legend.position = \"bottom\")  # Position legend at bottom\n\nplot_2\nplot_3\nplot_4\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Propotionality\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Inversely proporsional\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Combine variables\n\n\n\n\n\n\n\nFigure 3: EOQ model\n\n\n\n\nHolding costs are easy to understand (look at Figure 3 (a)); as you increase the order quantity, the more your inventory, therefore more holding costs. But what about transaction cost (Figure 3 (b)); imagine you increase the order of hoops for the brewery, the more you increase the order quantity the less transactions with your supplier, but it has a limit. Combine, the total costs (Figure 3 (c)), we can observe that there is minimum, which is point we want to archive; this minimum is in a big somewhat flat area, this is great for the practitioner (us)!, it means that there is room for being wrong and the same time not being so wrong. We can obtain the minimum by deriving total costs function over \\(Q\\). This is the derived equation (we will call it \\(Q^{*}\\), “q-star”):\n\\[\n\\frac{dC }{dQ} = Q^{*}  =  \\sqrt[2]{\\frac{2kd}{h}}\n\\tag{1}\\]\nThis means that in order to optimize the order quantity, \\(Q^*\\), you want to avoid the transaction costs by increasing the order quantity, BUT, you don’t want to overdo it because by increasing the \\(Q\\), you increase the holding costs \\(h\\) (there will just be too many inventory to handle). As with much in life, it is a balance.\nWe can replace \\(Q^{*}\\) in Equation 1 to find the minimum cost — we won’t do it.\nHowever, all of this is quite far from reality, and that’s okay!!! These kinds of models (deterministic models), in my opinion, are not meant to forecast with infinite precision the phenomena; instead they are meant to make some sense of it and build some intuition, which is often more valuable than doing the model itself. That’s why it is worth learning them."
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#the-normal-distribution-plus-words-of-caution",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#the-normal-distribution-plus-words-of-caution",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "The normal distribution plus words of caution",
    "text": "The normal distribution plus words of caution\nWe will be assuming that demand, \\(d\\), and lead time, \\(l\\), have a normal distribution. Therefore we have to know what the normal distribution is saying. If you know what a PDF, CDF, and quantiles are, you can skip this part.\n\nHyperparameters\nHere is a controversial opinion, I do not like the Normal distribution. I am done, I said it. It just tells there is an expected value, the mean \\(\\mu\\), and perturbations around it, the standard deviation \\(\\sigma^2\\). That phrasing can be applied to every single stuff, and I am very cautious when the universality principle can be applied, also in a way you are expressing ignorance because if you only know that there is perturbations around a mean, you don’t know how that perturbations are behaving, which is the idea, capture that uncertainty of the phenomena. Having said that, normal distributions are very helpful to make estimates despite its universality — also they are wildly use because of the central limit theorem which ultimately says that every phenomena will behave as a normal distribution as data is gathered to infinite. So assuming normality for demand, \\(d\\), and lead time, \\(l\\) variables:\n\\[\nd \\sim N(\\mu_{d}, \\sigma_{d}^{2})\n\\]\n\\[\nl\\sim N(\\mu_{l}, \\sigma_{l}^{2})\n\\]\n\n\n\n\n\n\nMore about skepticism about normal distributions can be found in Taleb (2007) book the Black Swan, term taken from the classical problem of inference, Hume’s problem. If you like science you should know it.\n\n\n\nWe can read the last equations as “demand and lead time follows a normal distribution of with hyperparameters mean \\(\\mu_x\\) and standard deviation \\(\\sigma^{2}_{x}\\)”. We will only care for the hyperparameters, nothing else, just to make it simple and not dealing with a lot of algebra, also because we can describe this distribution just with these hyperparameters, like in Figure 4.\n\n\nShow code\n# Computing the normal distribution\nx_norm &lt;- tibble(\n  x = seq(-6, 6, 0.01),\n  norm1 = dnorm(x, mean = 0, sd = 1), # this is called the \"standard\" normal distribution\n  norm2 = dnorm(x, mean = 0, sd = 3),\n  norm3 = dnorm(x, mean = 3, sd = 1)\n  # Look how we just modify the the hyperparameters\n)\n\nplot_5 &lt;- x_norm |&gt; \n  ggplot(aes(x = x)) +\n  geom_line(aes(y = norm1), color = \"blue\", size = 1) +\n  geom_line(aes(y = norm2), color = \"red\", size = 1) +\n  geom_line(aes(y = norm3), color = \"green\", size = 1) +\n  labs(\n    title = \"Normal distributions\",\n    y = \"Density\",\n    x = \"Probability\"\n  ) +\n  theme_bw()\n\nplot_5\n\n\n\n\n\n\n\n\nFigure 4: Three normal distributions\n\n\n\n\n\n\n\nPDF & CDF\nThe normal distribution is a Probability Density Function (PDF). That word, density, is important because it tell us that a normal distribution can take a continuous array of values; if we are saying that demand follows a normal distribution we are saying that it can take continuous values. This is not the case: “please give me 2.12419 kegs of beer”, that does not make sense. This is another assumption that we are explicitly making.\nAnother important subject is a Cumulative Density Function (CDF). Every PDF has its CDF. They are just the accumulative probability of of its relative PDF. Figure 5 shows both a PDF and CDF of a standardized normal distribution, \\(X \\sim N(\\mu = 0, \\sigma^2 = 1)\\). CDFs are important because they help access the risk of choosing a value within our normal distribution, or any PDF.\n\n\nShow code\n# Create data for both PDF and CDF\ndf_combined &lt;- tibble(\n  x = seq(-4, 4, 0.01),\n  pdf = dnorm(x),\n  cdf = pnorm(x)\n)\n\n\nplot_6 &lt;- ggplot(df_combined, aes(x = x)) +\n  geom_line(aes(y = pdf, color = \"PDF\"), linewidth = 1) +\n  geom_line(aes(y = cdf * max(pdf), color = \"CDF\"), linewidth = 1) +\n  scale_y_continuous(\n    name = \"Probability Density (PDF)\",\n    sec.axis = sec_axis(~./max(df_combined$pdf), \n                       name = \"Cumulative Probability (CDF)\",\n                       labels = scales::percent)\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"PDF\" = \"steelblue\", \"CDF\" = \"orange\")) +\n  labs(title = \"Standard Normal Distribution: PDF and CDF\",\n       x = \"Standard Deviations from Mean\",\n       color = \"Function\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\",\n    axis.title.y.right = element_text(color = \"orange\"),\n    axis.text.y.right = element_text(color = \"orange\")\n  )\n\nplot_6\n\n\n\n\n\n\n\n\nFigure 5: Normal PDF and CDF. Look how the mean is half of the CDF, so it’s saying that you capture half the probability of any given event that follows a normal PDF\n\n\n\n\n\n\n\nQuantiles & normal rule of thumb\nThere is another property of big importance. While CDFs describe the cumulative probability of a PDF within a range of values, quantiles describe specific points that divide the range of a PDF into equal probabilities. In a normal PDF there is a rule of thumb called 68–95–99.7 rule, which says that in the quantile \\(\\pm 1\\sigma\\) we capture 68% of probabilities of any normal distribution, in \\(\\pm 2\\sigma\\) 95% and \\(\\pm 3\\sigma\\) 99.7%.\n\n\nShow code\n# Create data for standard normal distribution\ndf &lt;- tibble(x = seq(-4, 4, length.out = 1000), y = dnorm(x))\n\n# Create the first plot with empirical rule\nplot_7 &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_line(linewidth = 1, color = \"black\") +\n  \n  # Add shaded regions for empirical rule\n  geom_area(data = df %&gt;% filter(x &gt;= -1 & x &lt;= 1), \n            aes(x = x, y = y), fill = \"lightblue\", alpha = 0.5) +\n  geom_area(data = df %&gt;% filter(x &gt;= -2 & x &lt;= 2), \n            aes(x = x, y = y), fill = \"lightgreen\", alpha = 0.3) +\n  geom_area(data = df %&gt;% filter(x &gt;= -3 & x &lt;= 3), \n            aes(x = x, y = y), fill = \"lightpink\", alpha = 0.2) +\n  \n  # Add vertical lines for mean and standard deviations\n  geom_vline(xintercept = 0, linetype = \"solid\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = c(-1, 1), linetype = \"dashed\", color = \"blue\") +\n  geom_vline(xintercept = c(-2, 2), linetype = \"dashed\", color = \"green\") +\n  geom_vline(xintercept = c(-3, 3), linetype = \"dashed\", color = \"purple\") +\n  \n  # Add annotations\n  annotate(\"text\", x = 0, y = 0.1, label = \"Mean (μ = 0)\", \n           color = \"red\", vjust = -1) +\n  annotate(\"text\", x = 0, y = 0.3, label = \"68% within ±1σ\", \n           color = \"blue\", size = 3) +\n  annotate(\"text\", x = 1.5, y = 0.25, label = \"95% within ±2σ\", \n           color = \"green\", size = 3) +\n  annotate(\"text\", x = 2.5, y = 0.2, label = \"99.7% within ±3σ\", \n           color = \"purple\", size = 3) +\n  \n  # Customize theme and labels\n  labs(title = \"Standard Normal Distribution with quantiles\",\n       subtitle = \"68-95-99.7 Rule (One, Two, Three Standard Deviations)\",\n       x = \"Standard Deviations from Mean\", \n       y = \"Density\") +\n  theme_bw() \n\n\nplot_7\n\n\n\n\n\n\n\n\nFigure 6: 68–95–99.7 rule. Notice how the quantiles divide the normal distribution in equal length.\n\n\n\n\n\nIf you think a quantile is just another way of describing a CDF, you are not alone, it is a common misconception. They are closely related and this relationship will become important. While a CDF function, \\(F(x)\\), computes the cumulative range of probabilities, \\(p\\), that are bigger or equal of \\(x\\), \\(P(X \\le  x) = F(x) = p\\); a quantile function computes the value, \\(x\\), to obtain a range of probabilities \\(p\\). In other words a quantile function is the inverse of the CDF function, \\(F^{-1}(p) = x\\). This is the explicit relationship:\n\\[\nF(x) = p  \\quad \\leftrightharpoons  \\quad F^{-1}(p) = x\n\\tag{2}\\]"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#stochastic-eoq-model",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#stochastic-eoq-model",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "Stochastic EOQ model",
    "text": "Stochastic EOQ model\nThe objective in transforming the EOQ model into a stochastic model is to increase the quality of our service level, which means serving our clients in time and full. Remember we are considering demand and lead time as random variables for the model.\n\\[\nd \\sim N(\\mu_{d}, \\sigma_{d}^{2})\n\\]\n\\[\nl\\sim N(\\mu_{l}, \\sigma_{l}^{2})\n\\]\n\nService Level (\\(\\alpha\\)) & Safety Stocks\nSafety stocks, \\(S_s\\), are a very useful buffer when we face an unusual demand. They are extra on hand inventory as a backup when we finish our predicted inventory level. They are related with the service level, \\(\\alpha\\), which is often expressed as percentage. How do they relate? Well by this seem to be complex equation:\n\\[\nS_{s} = Z_{\\alpha}\\sigma_{l_{d}}\n\\tag{3}\\]\nLet’s understand this equation\nFactor service level, \\(Z_{\\alpha}\\):\nWhat does it mean wanting to have 95% of service level, \\(\\alpha\\)?\nWe have periodic demand that follows a normal distribution.\n\\[\nd \\sim N(\\mu_{d}, \\sigma_{d})\n\\]\nIf we compute the CDF of \\(d\\) for any given value \\(x\\), it will provide us with the probability that will below of \\(x\\). We want to be extra sure about our computation of \\(x\\), we want 95% of confidence/probability about \\(x\\). We already know \\(d\\) (a.k.a the probability density of demand) and we know that our objective is 95% (a.k.a. \\(\\alpha\\)); we don’t want the CDF we want the quantile function.\n\\[\nF_{d}(z) = \\alpha \\rightarrow F_{d}^{-1} (\\alpha) = z\n\\tag{4}\\]\nThis is the same property as in Equation 2 but applied to the demand distribution. Equation 4 is equal to the standard normal quantile function (\\(\\Phi(\\alpha)\\))\n\\[\nF_{d}^{-1}(\\alpha) = \\Phi^{-1}(\\alpha) = z_{\\alpha}\n\\tag{5}\\]\n\n\n\n\n\n\n\\(d\\) can be computed from historical demand or by a forecast\n\n\n\nWhat about the other term, in Equation 3 ?:\n\n\n\n\n\n\nWarning\n\n\n\nMany probability theory terms coming. Read them, try to understand them. They are confusing. I would give at the end a way of how you can think of them intuitively.\n\n\n\\(Z_\\alpha\\) is just a factor term that will be multiplied by \\(\\sigma_{d_{l}}\\), which can be interpreted as the variability of demand, and lead time. Equation 3 is telling us that in order to determine the safety stocks we have to account the variability of two random variables \\(l\\) and \\(d\\). To do that we need the product of two random variables collapsing into a quantity that represents the integrated variability of \\(l\\) and \\(d\\). How to think about it: \\(\\sigma_{d_{l}}\\) is the area of risk of an inventory policy, the factor \\(Z_{\\alpha}\\) determines how much risk are we willing to minimize, the more we want to minimize (by increasing \\(\\alpha\\), service level), the more safety stocks, BUT, the more inventory costs by the holding costs. Something that you have to get clear is that safety stocks is the tool that help us battle the variability of the demand, making them essential to give an optimal service level.\n\n\n\n\n\n\nGetting more precise\n\n\n\nWith \\(\\sigma_{d_{l}}\\) I am not referring to joint PDF, instead I am referring to a product two random variables (in this case \\(\\sigma_{d}\\) and \\(\\sigma_{l}\\)) which does not create any new distribution, it just describes an area behind the intersection between this two random variables, \\(l\\) and \\(d\\), which accounts their variability, which would be multiplied by \\(Z_\\alpha\\) factor to make sure we are \\(\\alpha\\) secure of the computation of the safety stocks. If you want to know more about this derivation, search for “product of Gaussians” or “product of random variables”.\nI write this note because at the beginning I was confused between the difference of a joint PDF, a sum of random variables (convolution) and the product of random variables.\n\n\n\n\nIntegrating the (S, R) policy\nThe derivation of \\(\\sigma_{d_{l}}\\) depends on the inventory policy. The \\((S, R)\\) policy is particularly riskier than others because the risk time is the moment when you make an order for your suppliers, every \\(R\\) period, till the moment it arrives (every \\(l\\), lead time). So the risk period is: \\(R + l\\). I will show the derived equation:\n\\[\nS_{s} = Z_{\\alpha}\\sigma_{l_{d}} = Z_{\\alpha} \\sqrt{(\\mu_{l} + R)\\sigma_{d}^{2} + \\sigma_{l}^{2}\\mu_{d}^{2}}\n\\tag{6}\\]"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#setting-up-the-variables",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#setting-up-the-variables",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "1) Setting up the variables",
    "text": "1) Setting up the variables\nDr.House brewery wants to simulate its daily net inventory:\n\\[\nnet\\ inventory = on\\ hand\\ inventory\\ + in\\ transit\\ inventory\\ - backorders\n\\tag{7}\\]\nBut before starting they made some research and obtained the below parameters shown in the table. Notice the choice of using a service level, \\(\\alpha\\), of 95% (remember the importance of \\(\\alpha\\) for calculating safety stocks Equation 3).\n\n\nShow code\n# 1) Define Lead time and \nset.seed(2025)         # For reproducibility\nt &lt;- 365               # days\nd_mu &lt;- 100; l_mu = 4  # daily demand [units/day]\nd_std &lt;- 25; l_std = 1 # daily lead time \n\n# Parameters:\nparams &lt;- list(\n  k = 100, # fixed cost of a single transaction [$/order] \n  h = 0.5, # yearly holding costs for keeping one single piece in stock [$/unit * year]\n  d_yearly = 365 * d_mu, # yearly expected demand\n  alpha = 0.95\n)\n\n# Create the parameter table using gt package\nparameter_table &lt;- tibble(\n  Parameter = c(\"t\", \"d_mu\", \"l_mu\", \"d_std\", \"l_std\", \"k\", \"h\", \"d_yearly\", \"alpha\"),\n  Value = c(t, d_mu, l_mu, d_std, l_std, params$k, params$h, params$d_yearly, params$alpha),\n  Units = c(\"days\", \"units/day\", \"days\", \"units/day\", \"days\", \"$/order\", \"$/(unit*year)\", \"units/year\", \"dimensionless\"),\n  Meaning = c(\n    \"Time period (days)\",\n    \"Daily demand mean\",\n    \"Lead time mean\",\n    \"Daily demand standard deviation\",\n    \"Lead time standard deviation\",\n    \"Fixed cost per order\",\n    \"Yearly holding cost per unit\",\n    \"Yearly expected demand\",\n    \"Service level probability\"\n  )\n)\n\n# Create the gt table\ngt_table &lt;- parameter_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Inventory Simulation Parameters\",\n    subtitle = \"Model parameters and their definitions\"\n  ) %&gt;%\n  cols_label(\n    Parameter = \"Parameter\",\n    Value = \"Value\",\n    Units = \"Units\",\n    Meaning = \"Meaning\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Value),\n    decimals = 2,\n    drop_trailing_zeros = TRUE\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#f7f7f7\"),\n    locations = cells_body(rows = seq(1, nrow(parameter_table), by = 2))\n  ) %&gt;%\n  tab_options(\n    table.width = pct(95),\n    table.align = \"left\"\n  )\n\ngt_table\n\n\n\n\n\n\n\n\n\n\nInventory Simulation Parameters\n\n\nModel parameters and their definitions\n\n\nParameter\nValue\nUnits\nMeaning\n\n\n\n\nt\n365\ndays\nTime period (days)\n\n\nd_mu\n100\nunits/day\nDaily demand mean\n\n\nl_mu\n4\ndays\nLead time mean\n\n\nd_std\n25\nunits/day\nDaily demand standard deviation\n\n\nl_std\n1\ndays\nLead time standard deviation\n\n\nk\n100\n$/order\nFixed cost per order\n\n\nh\n0.5\n$/(unit*year)\nYearly holding cost per unit\n\n\nd_yearly\n36,500\nunits/year\nYearly expected demand\n\n\nalpha\n0.95\ndimensionless\nService level probability\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#estimating-s-and-r",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#estimating-s-and-r",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "2) Estimating S and R",
    "text": "2) Estimating S and R\nIn order to make the simulation we have to compute the \\(R\\) (review period) and \\(S\\) (order up-to level). In \\((S, R)\\) policy \\(S\\) depend on \\(R\\) given that to compute \\(S\\) you need to compute \\(S_s\\) (safety stocks) which depends on \\(R\\) given Equation 6.\n\\[\nR = \\frac{Q^{*}}{D} = \\sqrt{\\frac{2{k}}{hD}}\n\\tag{8}\\]\nLast equation Equation 8 was obtained with Equation 1. Next step is compute \\(Z_{\\alpha}\\) (service factor) by computing the quantile function of an standardize normal distribution,\\(\\Phi^{-1}(\\alpha)\\) (remember the relation of Equation 4).\n\\[\nZ_{\\alpha} = \\Phi^{-1}(\\alpha)\n\\]\nThen we compute Equation 6, shown below.\n\\[\nS_{s} = Z_{\\alpha}\\sigma_{l_{d}} = Z_{\\alpha} \\sqrt{(\\mu_{l} + R)\\sigma_{d}^{2} + \\sigma_{l}^{2}\\mu_{d}^{2}}\n\\]\nAs a last step from this part we compute \\(S\\):\n\\[\nS = d_{l} + d_{R} + S_{s} = \\mu_{l}R + \\mu_{d} + S_{s}\n\\]\nAll of this calculations are done in the code below.\n\n\nShow code\n# Review period (years -&gt; days)\nR &lt;- sqrt((2 * params$k) / (params$h * params$d_yearly)) # years between orders\nr_period &lt;- max(1, round(365 * R))                       # days between reviews (&gt;=1)\n\n# Safety stock and S\nz_alpha &lt;- qnorm(params$alpha)\nss &lt;- z_alpha * sqrt((l_mu + r_period) * d_std^2 + l_std^2 * d_mu^2)\nS &lt;- (l_mu + r_period) * d_mu + ss"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#initialize-vectors",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#initialize-vectors",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "3) Initialize vectors",
    "text": "3) Initialize vectors\nWe are going to run a for-loop like the one that produced Figure 1 but much more complex. As describe we want to simulate the net inventory, shown in Equation 7. But first let’s breaking down into parts and look at the code — it will make sense in the next section.\n\n\nShow code\nn_days &lt;- t\nmax_lead_time &lt;- ceiling(l_mu + 3 * l_std) + 5\n# in_transit rows are calendar days; we only store arrival quantities\nin_transit &lt;- matrix(0, nrow = n_days + max_lead_time, ncol = 1)\ncolnames(in_transit) &lt;- c(\"quantity\")\n\n# Pre-sample all daily demand\nd &lt;- pmax(0, round(rnorm(n_days, d_mu, d_std)))\n\n# Outputs of for-loop\non_hand &lt;- numeric(n_days)\nbackorders &lt;- numeric(n_days)\ninventory_position &lt;- numeric(n_days)  # on-hand + on-order - backorders\norders &lt;- numeric(n_days)\nlead_times &lt;- numeric(n_days)\n\n# initial state (before day 1)\non_hand_prev &lt;- S      # starting full to S\nbackorders_prev &lt;- 0\n\n\n\nA word about in-transit/on-order inventory\nIn-transit inventory, for the purposes of this simulation, works as a calendar that will keep track how many days has been since an order was made. More formally it is a calendar-based matrix of future scheduled arrivals. This is what makes the model to have memory about the lead-time variability, \\(\\sigma_{l}\\)."
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#for-loop-simulation",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#for-loop-simulation",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "4) For-loop simulation",
    "text": "4) For-loop simulation\nLet’s start with the friendly explication of the simulation:\nEvery loop is a day (for (day in 1:n_days){}). An inventory-day is divided in three:\n\nMornings:\n\nIn the morning we check whether there is any arrival from our suppliers arrival_qty &lt;- in_transit[day, \"quantity\"]\nAlso we check if we have any back-order backorders_prev &gt; 0. If we do, backorders_prev &gt; 0 = TRUE, then that means we haven’t had any on-hand inventory, which means that the only way to fulfill a back-order is if there is any arrival from the in-transit suppliers arrival_qty &gt; 0. If arrival_qty &gt; 0 = TRUE then we check if the inventory is enough to fulfill all back-orders (the condition is arrival_qty &gt;= backorders_prev), if not then you just subtract the remaining back-orders with the previous ones. The code for all the morning part is down below.\n\n\n\n\nShow code\nfor (day in 1:n_days) {\n  # arrivals this morning\n  arrival_qty &lt;- in_transit[day, \"quantity\"]\n  \n  # 1) allocate arrivals to existing bac-korders first\n  if (arrival_qty &gt; 0 && backorders_prev &gt; 0) {\n    if (arrival_qty &gt;= backorders_prev) {\n      arrival_remaining &lt;- arrival_qty - backorders_prev\n      backorders_cur &lt;- 0\n      on_hand_cur &lt;- on_hand_prev + arrival_remaining\n    } else {\n      # arrival not enough to clear back-orders\n      backorders_cur &lt;- backorders_prev - arrival_qty\n      on_hand_cur &lt;- on_hand_prev  # usually 0 when back-orders exist\n    }\n  } else {\n    # no back-orders or no arrivals\n    backorders_cur &lt;- backorders_prev\n    on_hand_cur &lt;- on_hand_prev + arrival_qty\n  }\n  #&gt;....\n  #&gt;....\n  #&gt;.... Next part, during the day\n}\n\n\n\nDuring the day.\n\nWe first ask us “Can we satisfy the today’s demand given our on-hand inventory? with the condition on_hand_cur &gt;= d[day].\n\n\n\n\nShow code\n  #&gt;....\n  #&gt;....\n  #&gt;....\n  if (on_hand_cur &gt;= d[day]) {\n    on_hand_cur &lt;- on_hand_cur - d[day]\n    # backorders_cur remains whatever it is (possibly 0)\n  } else {\n    # demand exceeds on-hand -&gt; create backorders\n    backorders_cur &lt;- backorders_cur + (d[day] - on_hand_cur)\n    on_hand_cur &lt;- 0\n  }\n  #&gt;....\n  #&gt;....\n  #&gt;....Next part, end of the day\n\n\n\nNights.\n\nWe keep track of in-transit inventory — aside from the daily routine, on_hand_cur &gt;= d[day] condition is a safeguard for the last day of the simulation. It prevents an error and ensures that once the model time-period is over, the model stops looking for future arrivals from the in-transit inventory.\nOur Excel with all the daily data has to be updated with the Equation 7 (net-inventory is the same as inventory_position)\nFinally we make a periodic review \\(R\\) if it is review time, day %% r_period == 0 = TRUE. If the condition is TRUE we have check the order quantity, the lead-time our suppliers, and approximate the arrival day (trusting in the word our suppliers); we keep track of new order with our in-transit calendar.\n\n\n\n\nShow code\nfor (day in 1:n_days) {\n  #&gt;....\n  #&gt;....\n  #&gt;....\n  # 3) compute on-order (future arrivals) and inventory position\n  if (day &lt; nrow(in_transit)) {\n    total_in_transit &lt;- sum(in_transit[(day + 1):nrow(in_transit), \"quantity\"])\n  } else {\n    total_in_transit &lt;- 0\n  }\n  inv_pos &lt;- on_hand_cur + total_in_transit - backorders_cur\n  \n  # save\n  on_hand[day] &lt;- on_hand_cur\n  backorders[day] &lt;- backorders_cur\n  inventory_position[day] &lt;- inv_pos\n  \n  # 4) periodic review: order up to S (inventory position used)\n  if (day %% r_period == 0) {\n    l_time &lt;- max(1, round(rnorm(1, l_mu, l_std)))\n    order_qty &lt;- max(0, S - inv_pos)\n    arrival_day &lt;- day + l_time\n    if (arrival_day &lt;= nrow(in_transit)) {\n      in_transit[arrival_day, \"quantity\"] &lt;- in_transit[arrival_day, \"quantity\"] + order_qty\n    } else {\n      # arrival beyond simulation time period/horizon — ignore or extend in_transit\n      # Here we ignore\n    }\n    orders[day] &lt;- order_qty\n    lead_times[day] &lt;- l_time\n    \n    cat(sprintf(\"Day %3d REVIEW: inv_pos = %.1f, order = %.0f, lead = %d (arrive day %d)\\n\",\n                day, inv_pos, order_qty, l_time, arrival_day))\n  }\n  \n  # update previous for next day\n  on_hand_prev &lt;- on_hand_cur\n  backorders_prev &lt;- backorders_cur\n}\n\n\nBelow the whole code:\n\n\nShow code\nfor (day in 1:n_days) {\n  # arrivals this morning\n  arrival_qty &lt;- in_transit[day, \"quantity\"]\n  \n  # 1) allocate arrivals to existing backorders first\n  if (arrival_qty &gt; 0 && backorders_prev &gt; 0) {\n    if (arrival_qty &gt;= backorders_prev) {\n      arrival_remaining &lt;- arrival_qty - backorders_prev\n      backorders_cur &lt;- 0\n      on_hand_cur &lt;- on_hand_prev + arrival_remaining\n    } else {\n      # arrival not enough to clear backorders\n      backorders_cur &lt;- backorders_prev - arrival_qty\n      on_hand_cur &lt;- on_hand_prev  # usually 0 when backorders exist\n    }\n  } else {\n    # no outstanding backorders or no arrivals\n    backorders_cur &lt;- backorders_prev\n    on_hand_cur &lt;- on_hand_prev + arrival_qty\n  }\n  \n  # 2) satisfy today's demand (d[day])\n  if (on_hand_cur &gt;= d[day]) {\n    on_hand_cur &lt;- on_hand_cur - d[day]\n    # backorders_cur remains whatever it is (possibly 0)\n  } else {\n    # demand exceeds on-hand -&gt; create backorders\n    backorders_cur &lt;- backorders_cur + (d[day] - on_hand_cur)\n    on_hand_cur &lt;- 0\n  }\n  \n  # 3) compute on-order (future arrivals) and inventory position\n  if (day &lt; nrow(in_transit)) {\n    total_in_transit &lt;- sum(in_transit[(day + 1):nrow(in_transit), \"quantity\"])\n  } else {\n    total_in_transit &lt;- 0\n  }\n  inv_pos &lt;- on_hand_cur + total_in_transit - backorders_cur\n  \n  # save\n  on_hand[day] &lt;- on_hand_cur\n  backorders[day] &lt;- backorders_cur\n  inventory_position[day] &lt;- inv_pos\n  \n  # 4) periodic review: order up to S (inventory position used)\n  if (day %% r_period == 0) {\n    l_time &lt;- max(1, round(rnorm(1, l_mu, l_std)))\n    order_qty &lt;- max(0, S - inv_pos)\n    arrival_day &lt;- day + l_time\n    if (arrival_day &lt;= nrow(in_transit)) {\n      in_transit[arrival_day, \"quantity\"] &lt;- in_transit[arrival_day, \"quantity\"] + order_qty\n    } else {\n      # arrival beyond simulation time period/horizon — ignore or extend in_transit\n      # Here we ignore\n    }\n    orders[day] &lt;- order_qty\n    lead_times[day] &lt;- l_time\n    \n    cat(sprintf(\"Day %3d REVIEW: inv_pos = %.1f, order = %.0f, lead = %d (arrive day %d)\\n\",\n                day, inv_pos, order_qty, l_time, arrival_day))\n  }\n  \n  # update previous for next day\n  on_hand_prev &lt;- on_hand_cur\n  backorders_prev &lt;- backorders_cur\n}\n\n\nDay  38 REVIEW: inv_pos = 1457.2, order = 3056, lead = 2 (arrive day 40)\nDay  76 REVIEW: inv_pos = 786.2, order = 3727, lead = 4 (arrive day 80)\nDay 114 REVIEW: inv_pos = 898.2, order = 3615, lead = 4 (arrive day 118)\nDay 152 REVIEW: inv_pos = 953.2, order = 3560, lead = 4 (arrive day 156)\nDay 190 REVIEW: inv_pos = 454.2, order = 4059, lead = 5 (arrive day 195)\nDay 228 REVIEW: inv_pos = 735.2, order = 3778, lead = 3 (arrive day 231)\nDay 266 REVIEW: inv_pos = 566.2, order = 3947, lead = 6 (arrive day 272)\nDay 304 REVIEW: inv_pos = 849.2, order = 3664, lead = 2 (arrive day 306)\nDay 342 REVIEW: inv_pos = 705.2, order = 3808, lead = 4 (arrive day 346)\n\n\nShow code\n# We compute a tibble data frame for a plot\nresults &lt;- tibble(\n  days = 1:n_days,\n  on_hand = on_hand,\n  backorders = backorders,\n  inventory_position = inventory_position,\n  demand = d,\n  order_qty = orders\n)\n\nresults |&gt; print(n = 10)\n\n\n# A tibble: 365 × 6\n    days on_hand backorders inventory_position demand order_qty\n   &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1     1   4298.          0              5243.    110         0\n 2     2   4200.          0              5145.     98         0\n 3     3   4109.          0              5054.     91         0\n 4     4   3991.          0              4936.    118         0\n 5     5   3901.          0              4846.     90         0\n 6     6   3845.          0              4790.     56         0\n 7     7   3756.          0              4701.     89         0\n 8     8   3637.          0              4582.    119         0\n 9     9   3510.          0              4455.    127         0\n10    10   3415.          0              4360.     95         0\n# ℹ 355 more rows\n\n\nPLOT!!!!!!\n\n\nShow code\nresults|&gt; \n  ggplot(aes(x = days)) +\n  geom_line(aes(y = on_hand), colour = \"#1E90FF\", size = 1) +\n  geom_line(aes(y = backorders), colour = \"red\", size = 1) +\n  theme_bw() +\n  labs(\n    title = \"Stochastic simulation of a (R, S) policy\",\n    subtitle = \"With alpha = 0.95\" \n  )+\n  geom_vline(xintercept = seq(0, n_days, r_period), \n             linetype = \"dashed\", alpha = 0.5, color = \"orange\", size = 1)+\n  annotate(\"text\", x = seq(0, n_days, r_period), y = s + 1, \n           label = \"Reorder\", size = 3, angle = 90, vjust = -0.5)\n\n\n\n\n\n\n\n\nFigure 8: 1st Simulation:In red is the back-orders and in blue the on-hand inventory\n\n\n\n\n\n\nLearning by playing\nLet’s simulate again everything but changing \\(\\alpha\\) to 50% and increase the variability of the lead-time \\(\\sigma^{2}_{l}\\) to 15.\n\n\nShow code\n# 1) Define Lead time and \nset.seed(2025)         # For reproducibility\nt &lt;- 365               # days\nd_mu &lt;- 100; l_mu = 4  # daily demand [units/day]\nd_std &lt;- 25; l_std = 15 # daily lead time \n\n# Parameters:\nparams2 &lt;- list(\n  k = 100, # fixed cost of a single transaction [$/order] \n  h = 0.5, # yearly holding costs for keeping one single piece in stock [$/unit * year]\n  d_yearly = 365 * d_mu, # yearly expected demand\n  alpha = 0.50\n)\n\n\n# Review period (years -&gt; days)\nR &lt;- sqrt((2 * params2$k) / (params2$h * params2$d_yearly))      # years between orders\nr_period &lt;- max(1, round(365 * R))                            # days between reviews (&gt;=1)\n\n# Safety stock and S\nz_alpha &lt;- qnorm(params2$alpha)\nss &lt;- z_alpha * sqrt((l_mu + r_period) * d_std^2 + l_std^2 * d_mu^2)\nS &lt;- (l_mu + r_period) * d_mu + ss\n\nn_days &lt;- t\nmax_lead_time &lt;- ceiling(l_mu + 3 * l_std) + 5\n# in_transit rows are calendar days; we only store arrival quantities\nin_transit &lt;- matrix(0, nrow = n_days + max_lead_time, ncol = 1)\ncolnames(in_transit) &lt;- c(\"quantity\")\n\n# Pre-sample all daily demand\nd &lt;- pmax(0, round(rnorm(n_days, d_mu, d_std)))\n\n# Outputs\non_hand &lt;- numeric(n_days)\nbackorders &lt;- numeric(n_days)\ninventory_position &lt;- numeric(n_days)  # on-hand + on-order - backorders\norders &lt;- numeric(n_days)\nlead_times &lt;- numeric(n_days)\n\n# initial state (before day 1)\non_hand_prev &lt;- S      # starting full to S\nbackorders_prev &lt;- 0\n\nfor (day in 1:n_days) {\n  # arrivals this morning\n  arrival_qty &lt;- in_transit[day, \"quantity\"]\n  \n  # 1) allocate arrivals to existing backorders first\n  if (arrival_qty &gt; 0 && backorders_prev &gt; 0) {\n    if (arrival_qty &gt;= backorders_prev) {\n      arrival_remaining &lt;- arrival_qty - backorders_prev\n      backorders_cur &lt;- 0\n      on_hand_cur &lt;- on_hand_prev + arrival_remaining\n    } else {\n      # arrival not enough to clear backorders\n      backorders_cur &lt;- backorders_prev - arrival_qty\n      on_hand_cur &lt;- on_hand_prev  # usually 0 when backorders exist\n    }\n  } else {\n    # no outstanding backorders or no arrivals\n    backorders_cur &lt;- backorders_prev\n    on_hand_cur &lt;- on_hand_prev + arrival_qty\n  }\n  \n  # 2) satisfy today's demand (d[day])\n  if (on_hand_cur &gt;= d[day]) {\n    on_hand_cur &lt;- on_hand_cur - d[day]\n    # backorders_cur remains whatever it is (possibly 0)\n  } else {\n    # demand exceeds on-hand -&gt; create backorders\n    backorders_cur &lt;- backorders_cur + (d[day] - on_hand_cur)\n    on_hand_cur &lt;- 0\n  }\n  \n  # 3) compute on-order (future arrivals) and inventory position\n  if (day &lt; nrow(in_transit)) {\n    total_in_transit &lt;- sum(in_transit[(day + 1):nrow(in_transit), \"quantity\"])\n  } else {\n    total_in_transit &lt;- 0\n  }\n  inv_pos &lt;- on_hand_cur + total_in_transit - backorders_cur\n  \n  # save\n  on_hand[day] &lt;- on_hand_cur\n  backorders[day] &lt;- backorders_cur\n  inventory_position[day] &lt;- inv_pos\n  \n  # 4) periodic review: order up to S (inventory position used)\n  if (day %% r_period == 0) {\n    l_time &lt;- max(1, round(rnorm(1, l_mu, l_std)))\n    order_qty &lt;- max(0, S - inv_pos)\n    arrival_day &lt;- day + l_time\n    if (arrival_day &lt;= nrow(in_transit)) {\n      in_transit[arrival_day, \"quantity\"] &lt;- in_transit[arrival_day, \"quantity\"] + order_qty\n    } else {\n      # arrival beyond simulation time period/horizon — ignore or extend in_transit\n      # Here we ignore\n    }\n    orders[day] &lt;- order_qty\n    lead_times[day] &lt;- l_time\n    \n    # cat(sprintf(\"Day %3d REVIEW: inv_pos = %.1f, order = %.0f, lead = %d (arrive day %d)\\n\",\n    #            day, inv_pos, order_qty, l_time, arrival_day))\n  }\n  \n  # update previous for next day\n  on_hand_prev &lt;- on_hand_cur\n  backorders_prev &lt;- backorders_cur\n}\n\nresults_2 &lt;- tibble(\n  days = 1:n_days,\n  on_hand = on_hand,\n  backorders = backorders,\n  inventory_position = inventory_position,\n  demand = d,\n  order_qty = orders\n)\n\n\nresults_2|&gt; \n  ggplot(aes(x = days)) +\n  geom_line(aes(y = on_hand), colour = \"blue\", size = 1) +\n  geom_line(aes(y = backorders), colour = \"red\", size = 1) +\n  theme_bw() +\n  labs(\n    title = \"Stochastic simulation of a (R, S) policy\",\n    subtitle = \"With alpha = 0.50\" \n  )+\n  geom_vline(xintercept = seq(0, n_days, r_period), \n             linetype = \"dashed\", alpha = 0.5, color = \"orange\", size = 1)+\n  annotate(\"text\", x = seq(0, n_days, r_period), y = s + 1, \n           label = \"Reorder\", size = 3, angle = 90, vjust = -0.5)\n\n\n\n\n\n\n\n\nFigure 9: 2ng Simulation: In red is the back-orders and in blue the on-hand inventory. Notice how much redish\n\n\n\n\n\nWow, compare Figure 8 with Figure 9. Changing \\(\\alpha\\) & \\(\\sigma^{2}_{l}\\) really makes the inventory more insecure. I suggest changing the hyperparameters of demand and lead-time, you will see big changes in the behavior of the simulation; at the same time change \\(\\alpha\\), you will notice that while increasing the variability of the model, keeping an \\(\\alpha\\) above 90% will really keep your inventory mostly safe."
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#sad-reality",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#sad-reality",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "Sad reality",
    "text": "Sad reality\nThere’s something sad around this last simulation. It also has assumptions that hurts its accuracy. Assumptions:\n\nNormality. Demand nor lead-time aren’t normal. They are discrete variables.\nIndependence. We are assuming that demand and lead-time are independent of each other, this can be untrue, we have to check their correlation.\nStationary. Demand has high autocorrelation, therefore it does not behave like in Figure 10.\nDifferent orders can cross each other.\n\n\n\nShow code\nplot8 &lt;- results |&gt; \n  ggplot(aes(x = days, y = demand)) +\n  geom_line(color = \"firebrick1\") +\n    theme_bw() +\n  labs(\n    title = \"Demand\",\n    subtitle = \"Stationarity demand is behaving\" \n  )\n\nplot8\n\n\n\n\n\n\n\n\nFigure 10: This kind of behavior isn’t real. There is high autocorrelation.\n\n\n\n\n\nAll of these makes the stochastic model with room for improvement — but not today."
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#p.d.",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#p.d.",
    "title": "Stochastic Inventory Modelling: A Tutorial for Dummies using R",
    "section": "P.D.",
    "text": "P.D.\nI thought this article would take me 2 days to finish. It took me 5. I am quite exhausted. I just love doing some math, and trying to make sense of the world and do crazy stuff. Anyone who came all the way to here, I hope you liked it; I know math can be hard, but it really is useful, and congrats if you made the effort, go right now to celebrate with your love ones. Cheers."
  },
  {
    "objectID": "blog/2025-09-24-bayesian-intuition/index.html",
    "href": "blog/2025-09-24-bayesian-intuition/index.html",
    "title": "Bayesian Intuition for Beginners with Interactive Web Applications",
    "section": "",
    "text": "My journey into statistics is an unusual case. I got an introduction of the classical test statistics (\\(z, \\space t, \\space tt, \\chi^{2}, etc...\\)) for experimental design class. I liked it a lot but I wasn’t satisfied and later on it felt that I needed a solid background of statistics; YouTube’s algorithm noticed it and showed an incredible video (Lennox 2016) that explains this school of thought of statistics called Bayesianism. It convinced me. The result: I learned Bayesian statistics first and then Frequentists stats.\nRecently I have been going deep into Frequentist approaches of decision making given that it is what must people know. But, I always get this feeling of insatisfaction, maybe even contempt. Reading a paper by Efron (1986) called “Why Isn’t Everyone a Bayesian?” — great article — inspired me to make my first Bayesian blog about the classical coin flipping scenario seeing from the lens of Bayesianism. It is really intuitive and playing with the web applications will make you have a better understanding. Hope you like it. My objective is to get you into the inverse probability rabbit hole.\nThis is for beginners and I won’t be using any math. If you want to know what is going on behind the scenario check Murphy (2023). The web applications for playing are in the extra links section at the right side of your the screen."
  },
  {
    "objectID": "blog/2025-09-24-bayesian-intuition/index.html#setting-up-your-belief",
    "href": "blog/2025-09-24-bayesian-intuition/index.html#setting-up-your-belief",
    "title": "Bayesian Intuition for Beginners with Interactive Web Applications",
    "section": "1) Setting up your belief",
    "text": "1) Setting up your belief\nYou know it is a dumb game because you know there is 50/50 chance of winning. The is no skill involvement. But okay, you proposed it so you have to play along. You will be betting on heads. Here as a Bayesian we define our beliefs of the probability that it falls heads, which is 50%:\n\n\n\n\n\n\n\n\n\nHere we are saying that you are 100% sure that the probability is 50%. This isn’t true. There is some variation produced from factory in the shape of the coin, or any other reason you can think about. So you define a more accurate belief:\n\n\n\n\n\n\n\n\n\nHere we are saying that we are still pretty sure that the probability is 50%. Imagine that we are really clueless, and we really don’t know. This is how we would represent our belief:\n\n\n\n\n\n\n\n\n\nWhy does this expression translates into cluelessness? By assigning equal probability to everything you are saying that anything can happen, it is an expression of complete uncertainty1\nHow do we modify our beliefs? We use something called probability density functions. The normal distribution is a probability density function. They contain something called hyperparameters that we use, in this case, to modify the state of our beliefs by changing its values. The next animation showz how the beta distribution, which is the probability density function we have used in the previous figures, changes when the hyperparameters are modified.\n\n\n\nBeta Distribution Animation\n\n\nNext it’s your turn to modified the hyperparameters and get a sense of how the beta distribution works. Go to the link play at the bottom right side of the screen with the name “Beta Distribution Visualizer”. Play with the app, and think how would YOU define your beliefs over a coin."
  },
  {
    "objectID": "blog/2025-09-24-bayesian-intuition/index.html#updating-your-belief",
    "href": "blog/2025-09-24-bayesian-intuition/index.html#updating-your-belief",
    "title": "Bayesian Intuition for Beginners with Interactive Web Applications",
    "section": "2) Updating your belief",
    "text": "2) Updating your belief\nThe game is about to start and suddenly a friendly voice says “I bet 100$ on Tails but with my coin”. You know that tone, it is your tricky cheater of a cousin. You know he has some biased coins in his closet. Your belief over the coin changes rapidly:\n\n\n\n\n\n\n\n\n\nYou just think he is going to cheat, and you are assigning a smaller probability of the coin been fair. The only way to change our belief is by playing, because maybe you are wrong and it is really a fair coin by landing 50/50 heads and tails. Let’s play 10 times:\n\n\n\n\n\n\n\n\n\nTurns out your prior belief, which stated he would cheat, isn’t correct. He used a fair coin. I guess sometimes people change."
  },
  {
    "objectID": "blog/2025-09-24-bayesian-intuition/index.html#bayesian-statistics",
    "href": "blog/2025-09-24-bayesian-intuition/index.html#bayesian-statistics",
    "title": "Bayesian Intuition for Beginners with Interactive Web Applications",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nThis is the essence of Bayesian statistics: a method in which we update our beliefs over something given evidence. The updated evidence is what is called the posterior and you can think of it as a compromise between our prior beliefs and the data.\n\\[\nposterior ≈ data \\space * \\space prior\n\\]\nPlease enter to the link at the bottom right side of the screen with the name “Bayesian Coin Toss Simulation” to get a sense how posterior is formed given evidence (data). If you have a very strong belief, it is probable that the prior overwhelms the data, so chose wisely your prior."
  },
  {
    "objectID": "blog/2025-09-24-bayesian-intuition/index.html#footnotes",
    "href": "blog/2025-09-24-bayesian-intuition/index.html#footnotes",
    "title": "Bayesian Intuition for Beginners with Interactive Web Applications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are better ways to express cluelessness in Bayesian statistics. This non-informative uniform prior is saying something at the end by assigning equal probabilities to anything. That’s something! There is others non-informative priors that really try not to say something, like Jeffreys prior. More info about noninformative priors in Murphy (2023)↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "",
    "text": "Last year I went crazy and started my statistics rabbit hole; one of the biggest mind blowing experiences was when I was taking a course called “Probabilistic Graphical Models” by Daphne Koller on Coursera and I understood the connection between these two classical modes: logistic regression (LG) and Naïve Bayes (NB). This is a fascinating topic and really you start to dig in about the nature of modelling, and how they resemble to Prague’s Golem; a metaphor taken from Richard McElreath’s book “Statistical Rethinking”, in which he writes “Its abundance of power is matched by its lack of wisdom” (McElreath 2020). Hope you enjoy it."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#bayesian-networksbns",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#bayesian-networksbns",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Bayesian Networks(BNs)",
    "text": "Bayesian Networks(BNs)\nImagine you are looking for an architect job and start wondering what qualities that companies look for. Your guess is they are looking for some technical drawing skills. So in a way you are saying that to be employed at an architectural firm (we will call it EA) depends on having good technical drawing skills (DS); in probability we say \\(EA\\) given (“|” it expresses dependency) \\(DS\\), and can be written as:\n\\[EA | DS\\]\nThis can be represented as a Graph (\\(G\\)):\n\n\n\n\n\nThis is call “Directed Graph” (in Graph Theory). Look at the direction of the arrow, it will become important later because it represents independency\n\n\n\n\nWe can write the probability (\\(P\\)) of \\(AF\\) as:\n\\[\nP(EA | DS)\n\\]\nBut you start to wonder that they also need candidates with architect-software skills (Sof), so you draw it:\n\n\n\n\n\nLook a the directions of the arrows, we are saying that EmpArch is dependent of that two nodes/variables (DS, Sof) and those two parameters are fully independent – because they do not have any arrow pointing at them. Embrace yourself, next paragraph this comes into play\n\n\n\n\nNow we say “\\(EA\\) is given by \\(DS\\) and \\(Sof\\)” (\\(EA | DS, Sof\\)). This is great, we have some notion of what to be employed as an architect depends on, this in itself, the \\(G\\) (graph), is a model; however the objective of BNs is to encapsulate the joint probability distribution of the model, which is to say that the objective of BNs is to provide the probability of a combination of variables (the correct term is random variables, but right now do not bother differentiating the two terms). In this case the joint probability distribution is written as \\(P(EA, DS, Sof)\\) and is equal to the conditional dependencies (this “\\(EA | DS\\)” is a conditional dependency) of all the variables it depends on, specified by the graph, so:\n\\[\nP(EA, DS, Sof) = P(EA | DS, Sof)P(DS)P(Sof)\n\\] Note how \\(DS\\) and \\(Sof\\) are fully independent (they do not have a “|”).\n\n\n\n\n\n\nNote\n\n\n\nFor the probabilistic known reader, we could derive Bayes’ rule from the last equation if we want to know the probability of getting an architect job given the X variables\n\n\nBut \\(Sof\\) encapsulates many kinds of softwares, like AutoCAD, 3ds Max, … Let’s say there is 3 principal softwares (\\(S_{1}, S_{2}, S_{3}\\)), then the \\(G\\) is modify:\n\n\n\n\n\nNow Sof isn’t independent, and S1, S2, S3 (software programs) are\n\n\n\n\nThis is the same as the joint probability:\n\\[\nP(EA, DS, Sof, S1, S2, S3) = P(EA | DS, Sof)P(DS)P(Sof |S_{1},S_{2}, S_{3})P(S_{1})P(S_{2})P(S_{3})\n\\]\nWe can go on to infinity creating this models, or \\(G\\), and they can get very tangled, but they are combination of dependencies and independencies. The mathematical representation of \\(BNs\\) for the joint probability distribution is:\n\\[\nP(X_{1},...,X_{n}) = \\prod_{i=1}^{n}P(x_{i}|Pa_{x_{i}}^{G})\n\\]\nWhere \\(X_i\\) represents the i variable (like Software Skills) and \\(Pa_{x_{i}}^{G}\\) means that it factorizes over the independencies. This equation is also called the chain rule for Bayesian networks – if you don’t know what \\(\\prod\\) means, it means the product. Fantastic, BNs are very intuitive, nonetheless they have many algorithmic implications as well as properties that are beyond the scope of this article."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#markov-networks-mn-a.k.a-markow-random-fields",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#markov-networks-mn-a.k.a-markow-random-fields",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Markov Networks (MN) a.k.a Markow Random Fields",
    "text": "Markov Networks (MN) a.k.a Markow Random Fields\nLet’s look first how BNs fail:\n\n\n\n\n\nCiclic Graph. The meaning of the variables of this example does not matter\n\n\n\n\nGiven this \\(G\\) we know that A is independent of B and B is independent of C and C is independent of A, isn’t it? Well here something odd happens; to see it let’s derive joint probability distribution using the chain rule of BNs:\n\\[\nP(A,B,C) = P(B|A)P(C|B)P(A|C) = P(B, C|A) P(A|C)=\n\\]\n\\[\n\\longrightarrow P(A,B,C | C) ????\n\\] What? Is C dependent on itself? It turns out you can derive the same expression but with A or B at the end, which would incorrectly suggest self-dependence. If we want to represent these kinds of interdependencies we need MNs.\nMNs do not have arrows and the lines that connect the nodes in the \\(G\\) does not represent dependencies/cause, they represent affinities/factors between all the nodes (\\(\\phi\\)). You can think of them as affinity functions. In this case:\n\\[P(A,B,C) \\approx \\phi(A,B)\\phi(A,C)\\phi(B,C)\\]\n\n\n\n\n\nIn Graph theory the graphs whithout arrows are known as undirected graphs.\n\n\n\n\nThe general equation isn’t very friendly: \\[\nP(X) = \\frac{1}{Z} \\prod_{c\\in clique } \\phi(c)\n\\]\nWhere \\(Z\\) is known as the partition function, which is just a normalization function (given it reduces the quantities to add up to 1) which equals to:\n\\[\nZ= \\sum_{c\\in clique } \\phi(c)\n\\]\nAbstract stuff. A clique is just the connections of sets (like \\(A,B,C\\)) that encapsulates an area in the \\(G\\). Last \\(G\\) just has one clique.\nI won’t continue explaining MNs; they do not come natural to our primates brains. Just keep in mind that they just represent affinities."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#am-i-naïve-or-just-stupid",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#am-i-naïve-or-just-stupid",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Am I naïve or just stupid?",
    "text": "Am I naïve or just stupid?\nMany models are made for prediction, where \\(Y\\) is our predictor and \\(X\\) is our features. In Nïve Bayes (NB) model we have a predictor, often written as \\(C\\), and a set of \\(n\\) features \\(X \\in (x_{1},..., x_{n})\\). This is the \\(G\\) of NB model:\n\n\n\n\n\nHere we are saying ther is n features for the C predictor. Do you see some resamble to some of the BNs we had worked?\n\n\n\n\nCan you guess what assumption (like dependencies or independencies) this \\(G\\) makes? (think about it)… This model is saying that in order to predict \\(C\\) we need variables that are independent of each other but all are dependent on \\(C\\) (In prbability theory independence is written as “\\(\\bot\\)”, so here the assumption is written as: \\((X_{i} \\bot X_{j} | C)\\)). This is why it is considered naïve; many times \\(X_i\\) and \\(X_j\\) aren’t independent. Imagine the case in which you want to predict happiness of people, you have just two variables: income —just for the sake of the example, although they are correlated—, and health. If you use NB you will not have accurate predictions, because you are assuming that \\((income \\bot health | happiness)\\) and this just isn’t the case!! In many countries income and health are correlated — just look how healthy poor people are in urban areas, and how rich people are.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you know some statistics or casuality, these kind of issues are more important in inference than in prediction because this arises what statisticians fear the most (suspense)…, confounders. Also, please do not think the naïvity of this model is just exclusive to this model.\n\n\nBefore showing the Naïve Bayes model, let’s remember the Chain Rule of BNs: \\[\nP(X_{1},...,X_{n}) = \\prod_{i=1}^{n}P(x_{i}|Pa_{x_{i}}^{G})\n\\]\nUsing our NB example of happiness:\n\\[\nP(happiness, income, health) =\n\\] \\[\n\\longrightarrow P(happiness)P(income|happiness)P(health|happiness)\n\\] This is the NB mathematical model. Easy, isn’t it? Now this is the generalized form (look how it remarks the assumption \\((X_{i} \\bot X_{j} | C)\\)):\n\\[\nP(Y, X) = P(Y)\\prod_{i=1}^{n}P(x_{i}|Y)\n\\]\nNow we know why it is naïve, what if I told you we can account for the dependencies of the model (like the dependency \\(health|rich\\)) without ruining the model or makeing it more complex. Behold."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#conditional-random-fields-crfs",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#conditional-random-fields-crfs",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Conditional Random Fields (CRFs)",
    "text": "Conditional Random Fields (CRFs)\nWe want to condition \\(Y\\) given \\(X\\), \\(P(Y|X)\\) so our model can account for all the dependencies — more technically, this means that we aren’t trying to capture the distribution of X, which is important in the NB model because we are estimating \\(P(x_{i}|Y)\\)– this is like not caring about the correlations/dependencies of \\(X\\). Trust me. This is where CRFs come into play.\n\n\n\n\n\n\nMany math gibberish coming, but it is necessary\n\n\n\nA CRF is defined as a MN, which uses a set of factors (\\(\\Phi = \\left\\{ \\phi_{1}(C_1),...,\\phi_{n}(C_n) \\right\\}\\)) (remember that they describe function of affinities, and \\(C\\) is the relation of two variables or scope) between variables:\n\\[\n\\tilde{P}_{\\Phi}(X, Y) =  \\prod_{i=1 } \\phi_{i}(c_{i})\n\\] \\(\\tilde{P}_{\\Phi}\\) is the same probabilities as described in the MNs section but anormalized (without the \\(\\frac{1}{Z}\\) term). However \\(Z\\) changes, it just becomes a function of \\(X\\) and sums over \\(Y\\):\n\\[\nZ_{\\Phi}(X) = \\sum_{Y}\\tilde{P}_{\\Phi}(X, Y)\n\\]\nThen:\n\\[\nP(Y|X) = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi}(X, Y)\n\\]\nThis different \\(Z_{\\Phi}(X)\\) isn’t just a normalizing constant, now depending on the value of \\(X\\) it will change \\(Z\\) for the \\(\\tilde{P}(X,Y)\\), it is like selection bias (if you are more pro you can think of it as creating a family of distributions given X), this property is what enables us to write the conditional \\(P(Y|X)\\) making us not care about the dependencies of features between the variables \\(X\\). This is what makes CRF such a powerful tool.\n\n\n\n\n\nLook how similar they are to the NB model, without the arrows. Remember the lines that connect the variables repesents affinities as in MN, BUT, in this case they are conditionally dependent on X given that Z, the normalizing constant, is a function of X, Z(X)"
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#playing-with-notation-logistic-regresion",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#playing-with-notation-logistic-regresion",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Playing with notation & logistic regresion",
    "text": "Playing with notation & logistic regresion\nComing back to the happiness example. Imagine you just have two values for happiness, \\(Y\\) (1 if you are happy, if not 0). Let’s write the logistic regression using an indicator function, \\(I\\) (an indicator function is just a conditional, pretty much like programming; like the happiness variable which takes two values: 1 if happy, 0 if not); with some features, \\(X_i\\) (either income or health).\n\\[\n\\phi_{i}(X_{i}, Y) = exp\\left\\{w_{i}I_{\\left\\{ X_{i} = 1, Y = 1 \\right\\}}\\right\\}\n\\]\nSo if \\(X_i = 1\\) and \\(Y = 1\\) then \\(I\\) (the indicator function) takes the value 1; given the term \\(w_i\\) (this kind of terms are usually called weighted terms because they assign some weight, meaning a higher value (imagine you meet the condition and the indicator function takes the value of 1, depending on the weight the whole expression can take a bigger or smaller value; e.g. \\((w_{1} = 0.5) \\neq (w_{2} = 0.7)\\) ), depending on \\(X_i\\) — very similar to the term \\(Z_{\\Phi}(X)\\) from CRFs, doesn’t it?)\nNow, imagine that we know \\(Y\\), we have two choices:\n\\[\n\\phi_{i}(X_{i}, Y = 1) = exp(w_{i}x_{i}) \\textbf{  or  } \\phi_{i}(X_{i}, Y = 0) = 1\n\\]\nSince \\(I\\) can either be 1 or 0, and if \\(I\\) is 0 then \\(exp(0)\\) which equals 1, \\(exp(0) = 1\\). Then we compute the unnormalized density (which in CRFs is the term \\(\\tilde{P_{\\Phi}}\\) which indicates it has not been normalized by \\(Z_{\\Phi}(X)\\) ):\n\\[\n\\tilde{P_{\\Phi}}(X, Y = 1) = exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\} \\text{ or } \\tilde{P_{\\Phi}}(X, Y = 0) = 1\n\\]\nNow normalized:\n\\[\nP_{\\Phi}(Y = 1 | X) = \\frac{exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\}}{1+exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\}}\n\\]\nIt is a sigmoid function as in logistic regression!!! Which is the same as:\n\\[\n\\longrightarrow \\frac{\\tilde{P_{\\Phi}}(X, Y = 1)}{\\tilde{P_{\\Phi}}(X, Y = 0) + \\tilde{P_{\\Phi}}(X, Y = 1)} = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi}(X, Y)\n\\]\n*If you still don’t see it remember that the normalizing value is: \\(Z_{\\Phi}(X) = \\sum_{Y}\\tilde{P}_{\\Phi}(X, Y)\\); the sum of all \\(\\tilde{P_{\\Phi}}(X, Y)\\) which in this case is a sum over two values.\n\n\n\n\n\nThis is just the same expression as the above!!\n\n\n\n\nSo a logistic regression (LR) is just a simple conditional random field (CRF) model!! Wow."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#p.d.-further-reading",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#p.d.-further-reading",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "P.D. & Further reading",
    "text": "P.D. & Further reading\nI love making the first article of my blog. I hope you liked it. If you want to know about more connections, like hidden markov networks and Linear-chain, I recommend this article see Sutton and McCallum (2010). This article was heavily based by the excellent book of Koller and Friedman (2009), which I am very keen of.\nMy adventures in statistics and modelling really have transformed how I see the world and has made me more humble; one of the objectives of this blog is to share some knowledge which is the least I can do. The code for the figures is on GitHub."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Bayesian Intuition for Beginners with Interactive Web Applications\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Inventory Modelling: A Tutorial for Dummies using R\n\n\n\n\n\nSep 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConnection Between Naïve Bayes and Logistic Regression\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\nNo matching items"
  }
]