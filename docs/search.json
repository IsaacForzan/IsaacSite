[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Forzán",
    "section": "",
    "text": "This webpage is done with the objective of advertise myself for companies and individuals to make me more hirable — or at least appealing. Think of it as a second résumé; however I would add the prefix anti-résumé because I also seek feedback, discussion, community to get better in my professional career.\nI am really interested in the world of modelling, biotech, engineering, technology, industry, and in a way philosophy (the fact of doing a mathematical model has many implications about reality itself). I’ll be posting about these topics.\nEmail: isaac_forzan@outlook.com"
  },
  {
    "objectID": "index.html#hola",
    "href": "index.html#hola",
    "title": "Isaac Forzán",
    "section": "",
    "text": "This webpage is done with the objective of advertise myself for companies and individuals to make me more hirable — or at least appealing. Think of it as a second résumé; however I would add the prefix anti-résumé because I also seek feedback, discussion, community to get better in my professional career.\nI am really interested in the world of modelling, biotech, engineering, technology, industry, and in a way philosophy (the fact of doing a mathematical model has many implications about reality itself). I’ll be posting about these topics.\nEmail: isaac_forzan@outlook.com"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html",
    "title": "Brewer stochastic inventory modelling. A tutorial for dummies using R",
    "section": "",
    "text": "Strangely enough, two years ago I was wondering in the college library thinking “what do quality engineers do?”; in my mind it’s common for me to be wondering that given the nature of my undergraduates degree (biotech engineer). Reading about it I stumbled with terms like Lean Six Sigma, DMAIC, SKUs, KPIs, etc…, but I wasn’t compelled because I read a word that make me turned my stomach. Gurus. I couldn’t at the time with that word. “How can a serious book for engineers can follow someone known as a guru” I thought. I wouldn’t have imagine that 2 years from that moment I would be digging in philosophies from this gurus — being Juran’s my favorite.\nA big part of quality is Inventory management, and that’s the topic for my second blog. This is the objective: teach you how to make a simulation of a beer business model for their inventory management using R + tidyverse. You cannot be vague nor lie to a computer; so if you program it, you know it.\nWe will be using the next packages:\n\n\nShow code\nlibrary(tidyverse) # for data manipulation, plotting, etc...\nlibrary(gt)        # for better tables\nlibrary(igraph)    # for graphs"
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#inventory-policy-periodic-review-order-up-to-level-r-s",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#inventory-policy-periodic-review-order-up-to-level-r-s",
    "title": "Brewer stochastic inventory modelling. A tutorial for dummies using R",
    "section": "Inventory policy: Periodic Review & Order Up-to Level (R, S)",
    "text": "Inventory policy: Periodic Review & Order Up-to Level (R, S)\nWhile there is many policies, Dr.House Brewer has a very popperian philosophy and do not trust new innovative policies that promise to increase their ROI by 100%, NO, the company adopts the common Periodic Review & Order Up-to Level policy. If you go to Walmart on a specific day of the week to supply yourself from the groceries, you are following Periodic Review & Order Up-to Level policy. It consists in a periodic time space (say every 5 workdays), \\(R\\), when you reorder all your inventory with your suppliers (fixed schedule) and an up-to level reorder point, \\(S\\) (you always want to have 10 eggs, you have right now 2, you wait until it is Monday (your reorder time, \\(R\\)) to buy your 8 eggs; 10 eggs is your up-to level reorder point, \\(S\\)). This policy is usually written as \\((R,S)\\), \\(R\\) for review period and \\(S\\) for up-to level reorder point.\n\nSimulation time: (R, S) policy in R\nWe will assume two things:\n\nThe demand of the customers, \\(d\\), is constant/fixed\nThe lead time for the materials to arrive the warehouse, \\(L\\), does NOT exists\n\nThese are strong assumptions that are going to be a key point latter in this blog. Lets think of \\(L = 0\\); if there is no lead time, \\(L\\), the moment it it is your reorder time \\(R\\), the moment you get it, this is never the case, but we are going to assume it for the moment.\nWe need to define the demand, \\(d\\), the time period (in this case daily), \\(R\\) and \\(S\\), so…,\n\n\nShow code\n# Parameters:\ns        &lt;- 14  # order-up-to level (14 units)\nr_period &lt;- 5   # review period (every 5 days)\ndemand   &lt;- 2   # constant daily demand\nn_days   &lt;- 24  # number of days to simulate\n \n# Initialize vectors for the simulation\ndays         &lt;- 1:n_days\ninventory    &lt;- numeric(n_days)  # the storage of the data\ninventory[1] &lt;- s                # Start with order-up-to level\n\n\nFantastic, we want a plot of how inventory changes across time using (R, S) policy, so it will be our dependent variable. I’ll show you the code, try to understand it, then I’ll explain the logic behind it:\n\n\nShow code\nfor (i in 1:n_days) {\n  if ((i - 1) %% r_period == 0 && i &gt; 1) { # \"%%\"-&gt; modulus operation so if 7 %% 3 = 1\n    # Replenish to order-up-to level, s:\n    inventory[i] &lt;- s                 \n  } else if (i &gt; 1){\n    # Carry forward previous day's ending inventory:\n    inventory[i] &lt;- inventory[i - 1]  \n  }\n  \n  # Consume demand at end of the day:\n  inventory[i] &lt;- inventory[i] - demand\n  # print it\n  cat(\"Day\", i, \": Inventory after demand =\", inventory[i], \"\\n\")\n}\n\n\nDay 1 : Inventory after demand = 12 \nDay 2 : Inventory after demand = 10 \nDay 3 : Inventory after demand = 8 \nDay 4 : Inventory after demand = 6 \nDay 5 : Inventory after demand = 4 \nDay 6 : Inventory after demand = 12 \nDay 7 : Inventory after demand = 10 \nDay 8 : Inventory after demand = 8 \nDay 9 : Inventory after demand = 6 \nDay 10 : Inventory after demand = 4 \nDay 11 : Inventory after demand = 12 \nDay 12 : Inventory after demand = 10 \nDay 13 : Inventory after demand = 8 \nDay 14 : Inventory after demand = 6 \nDay 15 : Inventory after demand = 4 \nDay 16 : Inventory after demand = 12 \nDay 17 : Inventory after demand = 10 \nDay 18 : Inventory after demand = 8 \nDay 19 : Inventory after demand = 6 \nDay 20 : Inventory after demand = 4 \nDay 21 : Inventory after demand = 12 \nDay 22 : Inventory after demand = 10 \nDay 23 : Inventory after demand = 8 \nDay 24 : Inventory after demand = 6 \n\n\nLogic: our inventory array will be decreasing per day given the constant demand, but given that we have a periodic review, \\(R\\), the moment it is \\(R\\)-day the moment it replenish your inventory (remember we won’t be waiting any time between order and and the suppliers get into our warehouse, there is no lead time) to a up-to level reorder point, \\(S\\). We need a for-loop that will help us evaluate whether the inventory per i day. A day is composed on mornings and evenings; in the morning we evaluate whether it is \\(R\\)-day or not, this is what it does the first condition (i - 1) %% r_period; inside the loop:\n\n\n\n\n\n\n\n\n\nDay\nEvaluation\nResult\nNote\n\n\n\n\ni = 1\n(1-1) %% 5 = 0\nFalse\nbut we exclude day 1 with && i &gt; 1\n\n\ni = 2\n(2-1) %% 5 = 1\nFalse\n-\n\n\ni = 3\n(3-1) %% 5 = 2\nFalse\n-\n\n\n.....\n.....\n.....\n.....\n\n\ni = 6\n(6-1) %% 5 = 0\nTrue\nequal to 0\n\n\n\nThe next conditional, else if (i &gt; 1) , just is saying “Is this NOT the first day AND NOT a review day?If YES, Start with yesterday’s leftover inventory”. Then by the end of the day we have inventory[i] - demand inventory.\n\n\n\n\n\n\nNote\n\n\n\nYou can make this for-loop a function with simulate_inventory &lt;- function(s, r_period, demand, n_days) as parameters, if you want.\n\n\n\n\nShow code\n# Create a tidy data fram, called tibble\ninventory_data &lt;- tibble(\n  days = days,\n  inventory = inventory\n)\n# Plot the inventory over time\nplot_1 &lt;- inventory_data |&gt; \n  ggplot(aes(days, inventory)) +\n  geom_line(size = 1, color = \"darkblue\", linetype = \"dashed\") +\n  geom_point(size = 2, color = \"red\") +\n  labs(\n    title = \"Inventory Levels Over Time\",\n    subtitle = paste(\"(R,S) Policy: Review every\", r_period, \"days, Order up to\", s, \"units\"),\n    x = \"Days\",\n    y = \"Inventory Level\"\n  ) +\n  scale_x_continuous(breaks = seq(0, n_days, 5)) +\n  theme_bw() +\n  geom_vline(xintercept = seq(5, n_days, r_period), \n             linetype = \"dashed\", alpha = 0.5, color = \"black\") +\n  annotate(\"text\", x = seq(5, n_days, r_period), y = s + 1, \n           label = \"Reorder\", size = 3, angle = 90, vjust = -0.5)\nplot_1 \n\n\n\n\n\n\n\n\nFigure 1: (R,S) simulation. Note how it never gets S = 14 [units]; this is given the simulation gives the inventory AFTER the review, or at the end of the day. So after R-day next day in the morning you have 14 but yuu will be consuming 2, given the daily demand, which makes it 12 [units] in the inventory\n\n\n\n\n\nFigure 1 shows the plot of inventory vs time. Note how the assumptions affects the simulation: there is an instantaneous replenishment of the inventory level and the inventory level is decreasing uniformly (just an straight line) given constant daily demand. This policy does not show fluctuations in time, but it can be risky because you can get into red levels of inventory level given an unexpected increase of demand and being far from \\(R\\)-day (or any given time-period).\nDr.House Brewer has chosen his policy, how can it be optimize?\n\n\n\n\n\n\nNote\n\n\n\nInventory Level is the net inventory: available on-hand inventory and in-transit inventory, minus backorders, orders not yet shipped, etc."
  },
  {
    "objectID": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#economic-order-quantity-eoq-model",
    "href": "blog/2058-09-04-stochastic-beer-inventory-case/index.html#economic-order-quantity-eoq-model",
    "title": "Brewer stochastic inventory modelling. A tutorial for dummies using R",
    "section": "Economic Order Quantity (EOQ) model",
    "text": "Economic Order Quantity (EOQ) model\nHow much should I order? turned out to be depending on the costs!!! — it is always about money.\n\\[\nC(Q)\n\\]\nWhere \\(C\\) is cost, and \\(Q\\) is order quantity. There is two types of costs:\n\nHolding costs. Costs associated with storing products. Two types:\nVariable holding costs (e.g. losses)\nFixed holding costs (e.g. employees)\nTransaction Costs. All costs associated with ordering the product from the suppliers. Not only transportation. Two types:\nVariable transaction costs (e.g. packaging)\nFixed transaction costs (e.g. fixed fees)\n\nWe won’t be caring for Variable transaction costs because it does not depend on \\(Q\\) (it depends on the demand, \\(d\\)). This is the equation:\n\\[\nTotal Costs = Holding Costs + Transaction Costs\n\\]\n\\[\n\\longrightarrow C(Q) = h \\frac {Q}{2} + k \\frac{D}{Q}\n\\]\n\n\n\n\n\n\nDimensional Analysis for the engineers out there:\n\n\n\n\\[\nC(Q) = \\left( \\frac {[currency]} {[unit]  [time]} * \\frac {[item]}{1} \\right) +  \\left( \\frac{[currency]}{[order]} * \\frac {[unit]}{[time]} * \\frac{1}{[unit]}      \\right)\n\\]\nSince \\([order]\\) is a count, it is often considered dimensionally equivalent to a pure number, adimensionally. So:\n\\[\nC(Q) = \\left( \\frac {[currency]} {[unit]  [time]} * \\frac {[item]}{1} \\right) +  \\left( \\frac{[currency]}{1} * \\frac {[unit]}{[time]} * \\frac{1}{[unit]}      \\right)\n\\]\n\\[\n= \\frac{[currency] }{[time]}  +  \\frac{[currency] }{[time]}  = \\frac{[currency] }{[time]}\n\\]\n\n\n\nWhat does it all mean?\nI won’t be focusing on the code, for the moment (it is relatively easy this chunk of code), because it isn’t really important. What it it is important is understanding EOQ equation (\\(C(Q)\\)). So let’s focus on the plots:\n\n\nShow code\n# EOC: total cost = holding costs + transactions costs\nh &lt;- 0.2\nk &lt;- 0.5 \n\neoc_m &lt;- tibble(\n    q = seq(1, 10, 0.1),         # Order quantity\n    h_cost = (h) * (q/2),        # Holding cost\n    t_cost = k * (demand/q),     # Transaction cost\n    total_cost = h_cost + t_cost # Total costs\n  ) \n\nplot_2 &lt;- eoc_m |&gt; \n  ggplot(aes(q, h_cost)) +\n  geom_line(color = \"darkorange\", size = 2) +\n  theme_bw() +\n  labs(\n    title = \"Holding cost: (h) * (q/2)\",\n    x = \"Cost\",\n    y = \"Order quantity\"\n  )\n\nplot_3 &lt;-  eoc_m |&gt; \n  ggplot(aes(q, t_cost))+\n  geom_line(color = \"cyan3\", size = 2) +\n  theme_bw() +\n  labs(\n    title = \"Transaction costs:  k * (demand/q)\",\n    x = \"Order quantity\",\n    y = \"Cost\"\n  )\n\n# Make it in long format for the labels in the ggplot\nplot_4 &lt;- eoc_m |&gt; pivot_longer(\n  cols = c(h_cost, t_cost, total_cost), \n  names_to = \"cost_type\", \n  values_to = \"cost_value\") |&gt; \n    mutate(cost_type = factor(cost_type, \n                              levels = c(\"h_cost\", \"t_cost\", \"total_cost\"),\n                              labels = c(\"Holding Cost\", \"Transaction Cost\", \"Total Cost\"))) |&gt; \n    ggplot(aes(x = q, y = cost_value, color = cost_type)) +\n    geom_line(size = 1) +\n    scale_color_manual(values = c(\"Holding Cost\" = \"darkorange\",\n                                  \"Transaction Cost\" = \"cyan3\",\n                                  \"Total Cost\" = \"dodgerblue4\")) +\n    theme_bw() +\n    labs(\n      x = \"Order quantity\",\n      y = \"Costs\",\n      color = \"Cost Type:\",\n      title = \"Economic Order Quantity (EOQ) Cost Analysis\"\n    ) +\n    geom_vline(xintercept = c(3, 3.5), \n               linetype = \"dashed\", alpha = 0.5, color = \"red\") +\n    annotate(\"text\", x = 3.3, y = 0.68, color = \"red\",\n             label = \"Optim. Area\", size = 3, vjust = -0.5) +\n    theme(legend.position = \"bottom\")  # Position legend at bottom\n\nplot_2\nplot_3\nplot_4\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Propotionality\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Inversely proporsional\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Combine variables\n\n\n\n\n\n\n\nFigure 3: EOC model\n\n\n\n\nHolding costs are easy to undertand (look at Figure 3 (a)) as you increase the order quantity, the more you inventory you have, therefore more holding costs. But what about transaction cost (Figure 3 (b)); imagine you increase the order of hoops for the brewery, the more you increase the order quantity the less transactions with your supplier, but it has a limit. Combine, the total costs (Figure 3 (c)), we can observe that there is minimum, which is point we want to archieve; this minimum is in a big somwhat flat area, this is great for the practioner (us)!, it means that there is room for being wrong and the same time not being so wrong. We can obtain the minimum by deriving total costs function over \\(Q\\). This is the derived equation (we will call it \\(Q^{*}\\), “q-star”):\n\\[\n\\frac{dC }{dQ} = Q^{*}  =  \\sqrt[2]{\\frac{2kd}{h}}\n\\tag{1}\\]\nThis means that in order to optimize the order quantity, \\(Q^*\\), you want to avoid the transaction costs by increasing the order quantity, BUT, you don’t want to overdo it because by increasing the \\(Q\\), by increasing \\(k\\), you increase the holding costs \\(h\\) (there will just be too many inventory to handle). As much in life, it is a balance.\nWe can replace \\(Q^{*}\\) in Equation 1\nHowever all of this is quite far from reality, and that’s okay!!! These kinds of models (deterministic models), in my opinion, are not meant to forecast with infinitely precision the phenomena, instead they are meant to make some sense of it and make some intuition, which is often more valuable than doing the model itself. That’s why it is worth learning it."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "",
    "text": "Last year I went crazy and started my statistics rabbit hole; one of the biggest mind blowing experiences was when I was taking a course called “Probabilistic Graphical Models” by Daphne Koller on Coursera and I understood the connection between these two classical modes: logistic regression (LG) and Naïve Bayes (NB). This is a fascinating topic and really you start to dig in about the nature of modelling, and how they resemble to Prague’s Golem; a metaphor taken from Richard McElreath’s book “Statistical Rethinking”, in which he writes “Its abundance of power is matched by its lack of wisdom” (McElreath 2020). Hope you enjoy it."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#bayesian-networksbns",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#bayesian-networksbns",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Bayesian Networks(BNs)",
    "text": "Bayesian Networks(BNs)\nImagine you are looking for an architect job and start wondering what qualities that companies look for. Your guess is they are looking for some technical drawing skills. So in a way you are saying that to be employed at an architectural firm (we will call it EA) depends on having good technical drawing skills (DS); in probability we say \\(EA\\) given (“|” it expresses dependency) \\(DS\\), and can be written as:\n\\[EA | DS\\]\nThis can be represented as a Graph (\\(G\\)):\n\n\n\n\n\nThis is call “Directed Graph” (in Graph Theory). Look at the direction of the arrow, it will become important later because it represents independency\n\n\n\n\nWe can write the probability (\\(P\\)) of \\(AF\\) as:\n\\[\nP(EA | DS)\n\\]\nBut you start to wonder that they also need candidates with architect-software skills (Sof), so you draw it:\n\n\n\n\n\nLook a the directions of the arrows, we are saying that EmpArch is dependent of that two nodes/variables (DS, Sof) and those two parameters are fully independent – because they do not have any arrow pointing at them. Embrace yourself, next paragraph this comes into play\n\n\n\n\nNow we say “\\(EA\\) is given by \\(DS\\) and \\(Sof\\)” (\\(EA | DS, Sof\\)). This is great, we have some notion of what to be employed as an architect depends on, this in itself, the \\(G\\) (graph), is a model; however the objective of BNs is to encapsulate the joint probability distribution of the model, which is to say that the objective of BNs is to provide the probability of a combination of variables (the correct term is random variables, but right now do not bother differentiating the two terms). In this case the joint probability distribution is written as \\(P(EA, DS, Sof)\\) and is equal to the conditional dependencies (this “\\(EA | DS\\)” is a conditional dependency) of all the variables it depends on, specified by the graph, so:\n\\[\nP(EA, DS, Sof) = P(EA | DS, Sof)P(DS)P(Sof)\n\\] Note how \\(DS\\) and \\(Sof\\) are fully independent (they do not have a “|”).\n\n\n\n\n\n\nNote\n\n\n\nFor the probabilistic known reader, we could derive Bayes’ rule from the last equation if we want to know the probability of getting an architect job given the X variables\n\n\nBut \\(Sof\\) encapsulates many kinds of softwares, like AutoCAD, 3ds Max, … Let’s say there is 3 principal softwares (\\(S_{1}, S_{2}, S_{3}\\)), then the \\(G\\) is modify:\n\n\n\n\n\nNow Sof isn’t independent, and S1, S2, S3 (software programs) are\n\n\n\n\nThis is the same as the joint probability:\n\\[\nP(EA, DS, Sof, S1, S2, S3) = P(EA | DS, Sof)P(DS)P(Sof |S_{1},S_{2}, S_{3})P(S_{1})P(S_{2})P(S_{3})\n\\]\nWe can go on to infinity creating this models, or \\(G\\), and they can get very tangled, but they are combination of dependencies and independencies. The mathematical representation of \\(BNs\\) for the joint probability distribution is:\n\\[\nP(X_{1},...,X_{n}) = \\prod_{i=1}^{n}P(x_{i}|Pa_{x_{i}}^{G})\n\\]\nWhere \\(X_i\\) represents the i variable (like Software Skills) and \\(Pa_{x_{i}}^{G}\\) means that it factorizes over the independencies. This equation is also called the chain rule for Bayesian networks – if you don’t know what \\(\\prod\\) means, it means the product. Fantastic, BNs are very intuitive, nonetheless they have many algorithmic implications as well as properties that are beyond the scope of this article."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#markov-networks-mn-a.k.a-markow-random-fields",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#markov-networks-mn-a.k.a-markow-random-fields",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Markov Networks (MN) a.k.a Markow Random Fields",
    "text": "Markov Networks (MN) a.k.a Markow Random Fields\nLet’s look first how BNs fail:\n\n\n\n\n\nCiclic Graph. The meaning of the variables of this example does not matter\n\n\n\n\nGiven this \\(G\\) we know that A is independent of B and B is independent of C and C is independent of A, isn’t it? Well here something odd happens; to see it let’s derive joint probability distribution using the chain rule of BNs:\n\\[\nP(A,B,C) = P(B|A)P(C|B)P(A|C) = P(B, C|A) P(A|C)=\n\\]\n\\[\n\\longrightarrow P(A,B,C | C) ????\n\\] What? Is C dependent on itself? It turns out you can derive the same expression but with A or B at the end, which would incorrectly suggest self-dependence. If we want to represent these kinds of interdependencies we need MNs.\nMNs do not have arrows and the lines that connect the nodes in the \\(G\\) does not represent dependencies/cause, they represent affinities/factors between all the nodes (\\(\\phi\\)). You can think of them as affinity functions. In this case:\n\\[P(A,B,C) \\approx \\phi(A,B)\\phi(A,C)\\phi(B,C)\\]\n\n\n\n\n\nIn Graph theory the graphs whithout arrows are known as undirected graphs.\n\n\n\n\nThe general equation isn’t very friendly: \\[\nP(X) = \\frac{1}{Z} \\prod_{c\\in clique } \\phi(c)\n\\]\nWhere \\(Z\\) is known as the partition function, which is just a normalization function (given it reduces the quantities to add up to 1) which equals to:\n\\[\nZ= \\sum_{c\\in clique } \\phi(c)\n\\]\nAbstract stuff. A clique is just the connections of sets (like \\(A,B,C\\)) that encapsulates an area in the \\(G\\). Last \\(G\\) just has one clique.\nI won’t continue explaining MNs; they do not come natural to our primates brains. Just keep in mind that they just represent affinities."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#am-i-naïve-or-just-stupid",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#am-i-naïve-or-just-stupid",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Am I naïve or just stupid?",
    "text": "Am I naïve or just stupid?\nMany models are made for prediction, where \\(Y\\) is our predictor and \\(X\\) is our features. In Nïve Bayes (NB) model we have a predictor, often written as \\(C\\), and a set of \\(n\\) features \\(X \\in (x_{1},..., x_{n})\\). This is the \\(G\\) of NB model:\n\n\n\n\n\nHere we are saying ther is n features for the C predictor. Do you see some resamble to some of the BNs we had worked?\n\n\n\n\nCan you guess what assumption (like dependencies or independencies) this \\(G\\) makes? (think about it)… This model is saying that in order to predict \\(C\\) we need variables that are independent of each other but all are dependent on \\(C\\) (In prbability theory independence is written as “\\(\\bot\\)”, so here the assumption is written as: \\((X_{i} \\bot X_{j} | C)\\)). This is why it is considered naïve; many times \\(X_i\\) and \\(X_j\\) aren’t independent. Imagine the case in which you want to predict happiness of people, you have just two variables: income —just for the sake of the example, although they are correlated—, and health. If you use NB you will not have accurate predictions, because you are assuming that \\((income \\bot health | happiness)\\) and this just isn’t the case!! In many countries income and health are correlated — just look how healthy poor people are in urban areas, and how rich people are.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you know some statistics or casuality, these kind of issues are more important in inference than in prediction because this arises what statisticians fear the most (suspense)…, confounders. Also, please do not think the naïvity of this model is just exclusive to this model.\n\n\nBefore showing the Naïve Bayes model, let’s remember the Chain Rule of BNs: \\[\nP(X_{1},...,X_{n}) = \\prod_{i=1}^{n}P(x_{i}|Pa_{x_{i}}^{G})\n\\]\nUsing our NB example of happiness:\n\\[\nP(happiness, income, health) =\n\\] \\[\n\\longrightarrow P(happiness)P(income|happiness)P(health|happiness)\n\\] This is the NB mathematical model. Easy, isn’t it? Now this is the generalized form (look how it remarks the assumption \\((X_{i} \\bot X_{j} | C)\\)):\n\\[\nP(Y, X) = P(Y)\\prod_{i=1}^{n}P(x_{i}|Y)\n\\]\nNow we know why it is naïve, what if I told you we can account for the dependencies of the model (like the dependency \\(health|rich\\)) without ruining the model or makeing it more complex. Behold."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#conditional-random-fields-crfs",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#conditional-random-fields-crfs",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Conditional Random Fields (CRFs)",
    "text": "Conditional Random Fields (CRFs)\nWe want to condition \\(Y\\) given \\(X\\), \\(P(Y|X)\\) so our model can account for all the dependencies — more technically, this means that we aren’t trying to capture the distribution of X, which is important in the NB model because we are estimating \\(P(x_{i}|Y)\\)– this is like not caring about the correlations/dependencies of \\(X\\). Trust me. This is where CRFs come into play.\n\n\n\n\n\n\nMany math gibberish coming, but it is necessary\n\n\n\nA CRF is defined as a MN, which uses a set of factors (\\(\\Phi = \\left\\{ \\phi_{1}(C_1),...,\\phi_{n}(C_n) \\right\\}\\)) (remember that they describe function of affinities, and \\(C\\) is the relation of two variables or scope) between variables:\n\\[\n\\tilde{P}_{\\Phi}(X, Y) =  \\prod_{i=1 } \\phi_{i}(c_{i})\n\\] \\(\\tilde{P}_{\\Phi}\\) is the same probabilities as described in the MNs section but anormalized (without the \\(\\frac{1}{Z}\\) term). However \\(Z\\) changes, it just becomes a function of \\(X\\) and sums over \\(Y\\):\n\\[\nZ_{\\Phi}(X) = \\sum_{Y}\\tilde{P}_{\\Phi}(X, Y)\n\\]\nThen:\n\\[\nP(Y|X) = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi}(X, Y)\n\\]\nThis different \\(Z_{\\Phi}(X)\\) isn’t just a normalizing constant, now depending on the value of \\(X\\) it will change \\(Z\\) for the \\(\\tilde{P}(X,Y)\\), it is like selection bias (if you are more pro you can think of it as creating a family of distributions given X), this property is what enables us to write the conditional \\(P(Y|X)\\) making us not care about the dependencies of features between the variables \\(X\\). This is what makes CRF such a powerful tool.\n\n\n\n\n\nLook how similar they are to the NB model, without the arrows. Remember the lines that connect the variables repesents affinities as in MN, BUT, in this case they are conditionally dependent on X given that Z, the normalizing constant, is a function of X, Z(X)"
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#playing-with-notation-logistic-regresion",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#playing-with-notation-logistic-regresion",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Playing with notation & logistic regresion",
    "text": "Playing with notation & logistic regresion\nComing back to the happiness example. Imagine you just have two values for happiness, \\(Y\\) (1 if you are happy, if not 0). Let’s write the logistic regression using an indicator function, \\(I\\) (an indicator function is just a conditional, pretty much like programming; like the happiness variable which takes two values: 1 if happy, 0 if not); with some features, \\(X_i\\) (either income or health).\n\\[\n\\phi_{i}(X_{i}, Y) = exp\\left\\{w_{i}I_{\\left\\{ X_{i} = 1, Y = 1 \\right\\}}\\right\\}\n\\]\nSo if \\(X_i = 1\\) and \\(Y = 1\\) then \\(I\\) (the indicator function) takes the value 1; given the term \\(w_i\\) (this kind of terms are usually called weighted terms because they assign some weight, meaning a higher value (imagine you meet the condition and the indicator function takes the value of 1, depending on the weight the whole expression can take a bigger or smaller value; e.g. \\((w_{1} = 0.5) \\neq (w_{2} = 0.7)\\) ), depending on \\(X_i\\) — very similar to the term \\(Z_{\\Phi}(X)\\) from CRFs, doesn’t it?)\nNow, imagine that we know \\(Y\\), we have two choices:\n\\[\n\\phi_{i}(X_{i}, Y = 1) = exp(w_{i}x_{i}) \\textbf{  or  } \\phi_{i}(X_{i}, Y = 0) = 1\n\\]\nSince \\(I\\) can either be 1 or 0, and if \\(I\\) is 0 then \\(exp(0)\\) which equals 1, \\(exp(0) = 1\\). Then we compute the unnormalized density (which in CRFs is the term \\(\\tilde{P_{\\Phi}}\\) which indicates it has not been normalized by \\(Z_{\\Phi}(X)\\) ):\n\\[\n\\tilde{P_{\\Phi}}(X, Y = 1) = exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\} \\text{ or } \\tilde{P_{\\Phi}}(X, Y = 0) = 1\n\\]\nNow normalized:\n\\[\nP_{\\Phi}(Y = 1 | X) = \\frac{exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\}}{1+exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\}}\n\\]\nIt is a sigmoid function as in logistic regression!!! Which is the same as:\n\\[\n\\longrightarrow \\frac{\\tilde{P_{\\Phi}}(X, Y = 1)}{\\tilde{P_{\\Phi}}(X, Y = 0) + \\tilde{P_{\\Phi}}(X, Y = 1)} = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi}(X, Y)\n\\]\n*If you still don’t see it remember that the normalizing value is: \\(Z_{\\Phi}(X) = \\sum_{Y}\\tilde{P}_{\\Phi}(X, Y)\\); the sum of all \\(\\tilde{P_{\\Phi}}(X, Y)\\) which in this case is a sum over two values.\n\n\n\n\n\nThis is just the same expression as the above!!\n\n\n\n\nSo a logistic regression (LR) is just a simple conditional random field (CRF) model!! Wow."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#p.d.-further-reading",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#p.d.-further-reading",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "P.D. & Further reading",
    "text": "P.D. & Further reading\nI love making the first article of my blog. I hope you liked it. If you want to know about more connections, like hidden markov networks and Linear-chain, I recommend this article see Sutton and McCallum (2010). This article was heavily based by the excellent book of Koller and Friedman (2009), which I am very keen of.\nMy adventures in statistics and modelling really have transformed how I see the world and has made me more humble; one of the objectives of this blog is to share some knowledge which is the least I can do. The code for the figures is on GitHub."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Brewer stochastic inventory modelling. A tutorial for dummies using R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnection Between Naïve Bayes and Logistic Regression\n\n\n\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\nNo matching items"
  }
]