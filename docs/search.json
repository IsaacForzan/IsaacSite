[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Forzán",
    "section": "",
    "text": "This webpage is done with the objective of advertise myself for companies and individuals to make me more hireable — or at least appealing. Think of it as a second resumé; however I would add the prefix anti-resumé because I also seek feedback, discussion, community to get better in my professional career.\nI am really intrested in the world of modelling, biotech, engineering, technology, industry, and in a way philosophy (the fact of doing a mathematical model has many implications about reality itself). I’ll be posting about this topics.\nEmail: isaac_forzan@outlook.com"
  },
  {
    "objectID": "index.html#hola",
    "href": "index.html#hola",
    "title": "Isaac Forzán",
    "section": "",
    "text": "This webpage is done with the objective of advertise myself for companies and individuals to make me more hireable — or at least appealing. Think of it as a second resumé; however I would add the prefix anti-resumé because I also seek feedback, discussion, community to get better in my professional career.\nI am really intrested in the world of modelling, biotech, engineering, technology, industry, and in a way philosophy (the fact of doing a mathematical model has many implications about reality itself). I’ll be posting about this topics.\nEmail: isaac_forzan@outlook.com"
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "",
    "text": "Last year I went crazy and started my statistics rabbit hole; one of the biggest mind blowing experiences was when I was taking a course called “Probabilistic Graphical Models” by Daphne Koller in Coursera and I understood the connection between these two classical modes: logistic regression (LG) and Naïve Bayes (NB). This is a fascinating topic and really you start to dig in about the nature of modelling, and how they resemble to Prague’s Golem; a metaphor taken from Richard McElreath’s book “Statistical Rethinking”, in which he writes “Its abundance of power is matched by its lack of wisdom” (McElreath 2020). Hope you enjoy it."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#bayesian-networksbns",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#bayesian-networksbns",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Bayesian Networks(BNs)",
    "text": "Bayesian Networks(BNs)\nImagine you are looking for an architect job and start wondering what qualities that companies look for. Your guess is they are looking for some technical drawing skills. So in a way you are saying that to be employed at a architects firm (we will call it EA) depends on having good technical drawing skills (DS); in probability we say \\(EA\\) given (“|” it expresses dependency) \\(DS\\), and can be written as:\n\\[EA | DS\\]\nThis can be represented as a Graph (\\(G\\)):\n\n\n\n\n\nThis is call “Directed Graph” (in Graph Theory). Look at the direction of the arrow, it will become important later because it represents independency\n\n\n\n\nWe can write the probability (\\(P\\)) of \\(AF\\) as:\n\\[\nP(EA | DS)\n\\]\nBut you start to wonder that they also need candidates with architect-software skills (Sof), so you draw it:\n\n\n\n\n\nLook a the directions of the arrows, we are saying that EmpArch is dependent of that two nodes/variables (DS, Sof) and those two parameters are fully independent – because they do not have any arrow pointing at them. Embrace yourself, next paragraph this comes into play\n\n\n\n\nNow we say “\\(EA\\) is given by \\(DS\\) and \\(Sof\\)” (\\(EA | DS, Sof\\)). This is great, we have some notion of what to be employed as an architect depends on, this in itself, the \\(G\\) (graph), is a model; however the objective of BNs is to encapsulate the joint probability distribution of the model, which is to say that the objective of BNs is to provide the probability of a combination of variables (the correct term is random variables, but right now do not bother differentiating the two terms). In this case the joint probability distribution is written as \\(P(EA, DS, Sof)\\) and is equal to the conditional dependencies (this “\\(EA | DS\\)” is a conditional dependency) of all the variables it depends on, specified by the graph, so:\n\\[\nP(EA, DS, Sof) = P(EA | DS, Sof)P(DS)P(Sof)\n\\] Note how \\(DS\\) and \\(Sof\\) are fully independent (they do not have a “|”).\n\n\n\n\n\n\nNote\n\n\n\nFor the probabilistic known reader, we could derive Bayes’ rule from last equation if we want to know the probability of getting an architect job given the X variables\n\n\nBut \\(Sof\\) encapsulates many kinds of softwares, like AutoCAD, 3ds Max, … Let’s say there is 3 principal softwares (\\(S_{1}, S_{2}, S_{3}\\)), then the \\(G\\) is modify:\n\n\n\n\n\nNow Sof isn’t independent, and S1, S2, S3 (software programs) are\n\n\n\n\nThis is the same as the joint probability:\n\\[\nP(EA, DS, Sof, S1, S2, S3) = P(EA | DS, Sof)P(DS)P(Sof |S_{1},S_{2}, S_{3})P(S_{1})P(S_{2})P(S_{3})\n\\]\nWe can go on to infinity creating this models, or \\(G\\), and they can get very tangled, but they are combination of dependencies and independencies. The mathematical representation of \\(BNs\\) for the joint probability distribution is:\n\\[\nP(X_{1},...,X_{n}) = \\prod_{i=1}^{n}P(x_{i}|Pa_{x_{i}}^{G})\n\\]\nWhere \\(X_i\\) represents the i variable (like Software Skills) and \\(Pa_{x_{i}}^{G}\\) means that it factorizes over the independencies. This equation is also called the chain rule for Bayesian networks – if you don’t know what \\(\\prod\\) means, it means the product. Fantastic, BNs are very intuitive, nonetheless they have many algorithmic implications as well as properties that are beyonf the scope of this article."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#markov-networks-mn-a.k.a-markow-random-fields",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#markov-networks-mn-a.k.a-markow-random-fields",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Markov Networks (MN) a.k.a Markow Random Fields",
    "text": "Markov Networks (MN) a.k.a Markow Random Fields\nLet’s look first how BNs fail:\n\n\n\n\n\nCiclic Graph. The meaning of the variables of this example does not matter\n\n\n\n\nGiven this \\(G\\) we know that A is independent of B and B is independent of C and C is independent of A, isn’t it? Well here something odd happens; to see it let’s derive joint probability distribution using the chain rule of BNs:\n\\[\nP(A,B,C) = P(B|A)P(C|B)P(A|C) = P(B, C|A) P(A|C)=\n\\]\n\\[\n\\longrightarrow P(A,B,C | C) ????\n\\] What? Is C dependent on itself? It turns out you can derive the same expression but with A or B at the end, which would incorrectly suggest self-dependence. If we want to represent these kinds of interdependencies we need MNs.\nMNs do not have arrows and the lines that connects the nodes in the \\(G\\) does not represent dependencies/cause, they represent affinities/factors between all the nodes (\\(\\phi\\)). You can think of them as affinity functions. In this case:\n\\[P(A,B,C) \\approx \\phi(A,B)\\phi(A,C)\\phi(B,C)\\]\n\n\n\n\n\nIn Graph theory the graphs whithout arrows are known as undirected graphs.\n\n\n\n\nThe general equation isn’t very friendly: \\[\nP(X) = \\frac{1}{Z} \\prod_{c\\in clique } \\phi(c)\n\\]\nWhere \\(Z\\) is known as the partition function, which is just a normalization function (given it reduces the quantities to add up to 1) which equals to:\n\\[\nZ= \\sum_{c\\in clique } \\phi(c)\n\\]\nAbstract stuff. A clique is just the connections of sets (like \\(A,B,C\\)) that encapsulates an area in the \\(G\\). Last \\(G\\) just have one clique.\nI won’t continue explaining MNs; they do not come natural to our primates brains. Just keep in mind that they just represent affinities."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#am-i-naïve-or-just-stupid",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#am-i-naïve-or-just-stupid",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Am I naïve or just stupid?",
    "text": "Am I naïve or just stupid?\nMany models are made for prediction, where \\(Y\\) is our predictor and \\(X\\) is our features. In Nïve Bayes (NB) model we have a predictor, often written as \\(C\\), and a set of \\(n\\) features \\(X \\in (x_{1},..., x_{n})\\). This is the \\(G\\) of NB model:\n\n\n\n\n\nHere we are saying ther is n features for the C predictor. Do you see some resamble to some of the BNs we had worked?\n\n\n\n\nCan you guess what assumption (like dependencies or independencies) this \\(G\\) makes? (think about it)… This model is saying that in order to predict \\(C\\) we need variables that independent of each other but all dependent on \\(C\\) (In prbability theory independence is written as “\\(\\bot\\)”, so here the assumption is written as: \\((X_{i} \\bot X_{j} | C)\\)). This is why it is considered naïve; many times \\(X_i\\) and \\(X_j\\) aren’t independent. Imagine the case in which you want to predict happiness of people, you have just two variables: income —just for the sake of the example, although they are correlated—, and health. If you use NB you will not have accurate predictions, because you are assuming that \\((income \\bot health | happiness)\\) and this just isn’t the case!! In many countries the income and health are correlated — just look how healthy poor people are in urban areas, and how rich people are.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you know some statistics or casuality, these kinds of issues are more important in inference than in prediction because this arises what statisticians fear the most (suspense)…, confounders. Also, please do not think the naïvity of this model is just exclusive to this model.\n\n\nBefore showing the Naïve Bayes model, let’s remember the Chain Rule of BNs: \\[\nP(X_{1},...,X_{n}) = \\prod_{i=1}^{n}P(x_{i}|Pa_{x_{i}}^{G})\n\\]\nUsing our NB example of happiness:\n\\[\nP(happiness, income, health) =\n\\] \\[\n\\longrightarrow P(happiness)P(income|happiness)P(health|happiness)\n\\] This is the NB mathematical model. Easy, isn’t it? Now this is the generalized form (look how it remarks the assumption \\((X_{i} \\bot X_{j} | C)\\)):\n\\[\nP(Y, X) = P(Y)\\prod_{i=1}^{n}P(x_{i}|Y)\n\\]\nNow we know why it is naïve, what if I told you we can account for the dependencies of the model (like the dependency \\(health|rich\\)) without ruining the model or makeing it more complex. Behold."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#conditional-random-fields-crfs",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#conditional-random-fields-crfs",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Conditional Random Fields (CRFs)",
    "text": "Conditional Random Fields (CRFs)\nWe want to condition \\(Y\\) given \\(X\\), \\(P(Y|X)\\) so our model can account for all the dependencies — more technically, this means that we aren’t trying to capture the distribution of X, which is important in the NB model because we are estimating \\(P(x_{i}|Y)\\)– this is like not caring about the correlations/dependencies of \\(X\\).Trust me. This is where CRFs come into play.\n\n\n\n\n\n\nMany math gibberish coming, but it is necessary\n\n\n\nA CRF is define as a MN, which uses a set of factors (\\(\\Phi = \\left\\{ \\phi_{1}(C_1),...,\\phi_{n}(C_n) \\right\\}\\)) (remember that they describe function of affinities, and \\(C\\) is relation of two variables or scope) between variables:\n\\[\n\\tilde{P}_{\\Phi}(X, Y) =  \\prod_{i=1 } \\phi_{i}(c_{i})\n\\] \\(\\tilde{P}_{\\Phi}\\) is the same probabilities as described in the MNs section but anormalized (without the \\(\\frac{1}{Z}\\) term).However \\(Z\\) changes, it just becomes a function of \\(X\\) and sums over \\(Y\\):\n\\[\nZ_{\\Phi}(X) = \\sum_{Y}\\tilde{P}_{\\Phi}(X, Y)\n\\]\nThen:\n\\[\nP(Y|X) = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi}(X, Y)\n\\]\nThis different \\(Z_{\\Phi}(X)\\) isn’t just a normalizing constant, now depending on the value of \\(X\\) it will change \\(Z\\) for the \\(\\tilde{P}(X,Y)\\), it is like selection bias (if you are more pro you can think of it as creating a family of distributions given X), this property is what enable us to write the conditional \\(P(Y|X)\\) making us not care about the dependencies of features between the variables \\(X\\). This is what makes CRF such a powerful tool.\n\n\n\n\n\nLook how similar they are to the NB model, without the arrows. Remember the lines that connect the variables repesents affinities as in MN, BUT, in this case they are conditionally dependent on X given that Z, the normalizing constant, is a function of X, Z(X)"
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#playing-with-notation-logistic-regresion",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#playing-with-notation-logistic-regresion",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "Playing with notation & logistic regresion",
    "text": "Playing with notation & logistic regresion\nComing back to the happiness example. Imagine you just have two values for happiness, \\(Y\\) (1 if you are happy, if not 0). Let’s write the logistic regression using an indicator function, \\(I\\) (an indicator function is just a conditional, pretty much like programming; like the happiness variable which takes two values: 1 if happy, 0 if not); with some features, \\(X_i\\) (either income or health).\n\\[\n\\phi_{i}(X_{i}, Y) = exp\\left\\{w_{i}I_{\\left\\{ X_{i} = 1, Y = 1 \\right\\}}\\right\\}\n\\]\nSo if \\(X_i = 1\\) and \\(Y = 1\\) then \\(I\\) (the indicator function) takes the value 1; given the term \\(w_i\\) (this kind of terms are usually called weighted terms because they assign some weight, meaning a higher value (imagine you meet the condition and the indicator function takes the value of 1, depending on the weight the whole expression can take a bigger or smaller value; e.g. \\((w_{1} = 0.5) \\neq (w_{2} = 0.7)\\) ), depending on \\(X_i\\) — very similar to the term \\(Z_{\\Phi}(X)\\) from CRFs, doesn’t it?)\nNow, imagine that we know \\(Y\\), we have two choices:\n\\[\n\\phi_{i}(X_{i}, Y = 1) = exp(w_{i}x_{i}) \\textbf{  or  } \\phi_{i}(X_{i}, Y = 0) = 1\n\\]\nSince \\(I\\) can either be 1 or 0, and if \\(I\\) is 0 then \\(exp(0)\\) which equals 1, \\(exp(0) = 1\\). Then we compute the unnormalized density (which in CRFs is the term \\(\\tilde{P_{\\Phi}}\\) which indicates it has not been normalized by \\(Z_{\\Phi}(X)\\) ):\n\\[\n\\tilde{P_{\\Phi}}(X, Y = 1) = exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\} \\text{ or } \\tilde{P_{\\Phi}}(X, Y = 0) = 1\n\\]\nNow normalized:\n\\[\nP_{\\Phi}(Y = 1 | X) = \\frac{exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\}}{1+exp\\left\\{ \\sum_{i}w_{i}X_{i} \\right\\}}\n\\]\nIt is a sigmoid function as in logistic regression!!! Which is the same as:\n\\[\n\\longrightarrow \\frac{\\tilde{P_{\\Phi}}(X, Y = 1)}{\\tilde{P_{\\Phi}}(X, Y = 0) + \\tilde{P_{\\Phi}}(X, Y = 1)} = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi}(X, Y)\n\\]\n*If you still don’t see it remember that the normalizing value is: \\(Z_{\\Phi}(X) = \\sum_{Y}\\tilde{P}_{\\Phi}(X, Y)\\); the sum of all \\(\\tilde{P_{\\Phi}}(X, Y)\\) which in this case is a sum over two values.\n\n\n\n\n\nThis is just the same expression as the above!!\n\n\n\n\nSo a logistic regression (LR) is just a simple conditional random field (CRF) model!! Wow."
  },
  {
    "objectID": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#p.d.-further-reading",
    "href": "blog/2058-08-26-connection-between-logistic-regression-and-naive-bayes/index.html#p.d.-further-reading",
    "title": "Connection Between Naïve Bayes and Logistic Regression",
    "section": "P.D. & Further reading",
    "text": "P.D. & Further reading\nI love making the first article of my blog. I hope you liked it. If you want to know about more connections, like hidden markov networks and Linear-chain, I recommend this article see Sutton and McCallum (2010). This article was heavily based by the excellent book of Koller and Friedman (2009), which I am very keen of.\nMy adventures in statistics and modelling really have transformed how I see the world and has made me more humble; one of the objectives of this blog is to share some knowledge which is the least I can do. The code for the figures is on GitHub."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Connection Between Naïve Bayes and Logistic Regression\n\n\n\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\nNo matching items"
  }
]